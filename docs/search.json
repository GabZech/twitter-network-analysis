[
  {
    "objectID": "src/analysis.html",
    "href": "src/analysis.html",
    "title": "Twitter Network Analysis",
    "section": "",
    "text": "First, let’s define some helper functions to be used later.\n\n\nCode\nimport json\nimport re\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_bar(x, y, title=None, xtick=None, rotation=0, xlabel=None, ylabel=None, width=0.8, horizontal=False):\n    if horizontal:\n        plt.barh(y=x, width=y, height=width)\n        xlabel, ylabel = ylabel, xlabel\n        plt.gca().invert_yaxis()\n    else:\n        plt.bar(x=x, height=y, width=width)\n    plt.xticks(xtick, rotation=rotation)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.title(title)\n\ndef extract_missing_usernames(df, username_column):\n    pattern = r\"RT @([A-Za-z0-9_]+):\"\n    usernames = []\n    for index, row in df.iterrows():\n        match = re.search(pattern, row[\"retweet_text\"])\n        if match:\n            usernames.append(match.group(1))\n        else:\n            usernames.append(row[username_column])\n    df[username_column] = usernames\n\n    return df\n\n\ndef get_time_range(df):\n    \"\"\"Get the time range of the DataFrame\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame to be queried\n    date_lang : str, optional\n        Language of the date, by default \"de_DE\"\n\n    Returns\n    -------\n    str\n        Start date of the DataFrame\n    str\n        End date of the DataFrame\n    \"\"\"\n    df.retweet_created_at = df.retweet_created_at.astype(\"datetime64[ns, UTC]\")\n    start_date = df.retweet_created_at.min().strftime(\"%B %e, %Y\")\n    end_date = df.retweet_created_at.max().strftime(\"%B %e, %Y\")\n\n    return start_date, end_date\n\n\ndef get_largest_values(df, col_name, n):\n    \"\"\"Get the n largest values of a column in a DataFrame\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame to be queried\n    col_name : str\n        Name of the column to be queried\n    n : int\n        Number of largest values to be returned (i.e. number of rows)\n\n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame with the n largest values of the column\n    \"\"\"\n    top = df.sort_values(col_name, ascending=False).head(n)\n\n    # put col_name as first column\n    cols = top.columns.tolist()\n    cols = cols[-1:] + cols[:-1]\n    top = top[cols]\n\n    return top\n\n\ndef get_top_users(df, df_authors, column_name, n):\n    \"\"\"Get the top n users with their profiles based on a column in a DataFrame\n    Values of the column are standardized so that the largest value is 1.0\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame to be queried\n    df_authors : pandas.DataFrame\n        DataFrame with the usernames and names of authors\n    column_name : str\n        Name of the column to be queried\n    n : int\n        Number of largest values to be returned (i.e. number of rows)\n\n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame with the n largest values of the column\n    \"\"\"\n    column_name_std = column_name + \" (normalised)\"\n    df[column_name_std] = df[column_name] / max(df[column_name])\n    df = get_largest_values(df, column_name, n)\n    df = add_profile_url(df, \"username\")\n    df = pd.merge(df, df_authors, on=\"username\", how=\"left\")\n    df = df.round(5)\n    df.index = np.arange(1, len(df) + 1)\n    df = df[[column_name, column_name_std, \"username\", \"name\", \"profile_url\"]]\n    return df\n\n\ndef get_authors_name(df):\n    \"\"\"Get the usernames and names of retweet authors and tweet authors\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame to be queried\n\n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame with the usernames and names of retweet authors and tweet authors\n    \"\"\"\n    retweet_authors = df[[\"retweet_author_username\", \"retweet_author_name\"]].copy()\n    retweet_authors.rename(\n        columns={\"retweet_author_username\": \"username\", \"retweet_author_name\": \"name\"},\n        inplace=True,\n    )\n\n    tweet_authors = df[[\"tweet_author_username\", \"tweet_author_name\"]].copy()\n    tweet_authors.rename(\n        columns={\"tweet_author_username\": \"username\", \"tweet_author_name\": \"name\"},\n        inplace=True,\n    )\n\n    authors = pd.concat([retweet_authors, tweet_authors])\n    authors = authors.drop_duplicates(subset=[\"username\"], keep=\"last\").reset_index(\n        drop=True\n    )\n\n    return authors\n\n\ndef add_profile_url(df, username_col):\n    df[\"profile_url\"] = \"https://twitter.com/\" + df[username_col]\n\n    return df\n\n\nNow let’s prepare the dataset and print some relevant information about it.\n\n\nCode\n# load and clean dataset\ndf = pd.read_parquet(\"data/raw/all_tweets_lehrkraeftebildung.parquet\")\ndf.replace([\"NaN\", \"nan\", \"None\", \"\"], np.NaN, inplace=True)\ndf = extract_missing_usernames(df, \"tweet_author_username\")\n\n# get information about the retweets\nstart_date, end_date = get_time_range(df)\nsearch_words = \"(Lehrkräftebildung OR Lehrerbildung OR Lehrkräfte OR Lehrkräftefortbildung OR Seiteneinstieg OR Quereinstieg OR Lehramt)\"\nquery_conds = \"(is:retweet OR is:quote) lang:de\"\n\n# drop retweets with missing usernames\nold_df_len = df.shape[0]\ntry:\n    missing_usernames = df.tweet_author_username.isnull().value_counts()[True]\nexcept KeyError:\n    missing_usernames = 0\ndf = df.dropna(subset=[\"tweet_author_username\"])\n\n\n# Print info about dataset\nprint(f\"Number of total retweets in this dataset: \\n{old_df_len}\")\nprint(f\"\\nTime range of the retweets:\\n{start_date} - {end_date}\")\nprint(f\"\\nKeywords* used to collect the retweets:\\n{search_words}\")\nprint(f\"\\nQuery conditions used to collect the retweets:\\n{query_conds}\")\nprint(f\"\\nNumber of retweets with missing usernames for the original tweeter: {missing_usernames}\\nThese are being dropped from the analysis. New total of retweets: {len(df)}\")\n\n\nNumber of total retweets in this dataset: \n11027\n\nTime range of the retweets:\nFebruary 23, 2023 - April  1, 2023\n\nKeywords* used to collect the retweets:\n(Lehrkräftebildung OR Lehrerbildung OR Lehrkräfte OR Lehrkräftefortbildung OR Seiteneinstieg OR Quereinstieg OR Lehramt)\n\nQuery conditions used to collect the retweets:\n(is:retweet OR is:quote) lang:de\n\nNumber of retweets with missing usernames for the original tweeter: 199\nThese are being dropped from the analysis. New total of retweets: 10828\n\n\nNow let’s prepare the graph.\n\n\nCode\nimport graph_tool.all as gt\n\n# prepare dataset for graph\ndf_ = df.copy()\ndf_[\"weight\"] = df_.groupby(['retweet_author_username', 'tweet_author_username']).transform('size')\n\n# choose columns to keep\ncolumns = ['retweet_author_username',\n    'tweet_author_username',\n    \"weight\",\n    ]\n\ndf_ = df_[columns].drop_duplicates(subset=['retweet_author_username', 'tweet_author_username'])\n\n# create list of edges\ng_list = [(r, t, w, r, t) for r, t, w in zip(df_.retweet_author_username, df_.tweet_author_username, df_.weight)]\n\n# create graph\ngf = gt.Graph(\n    g_list,\n    hashed=True,\n    eprops=[('weight', 'int'), ('retweeter', 'string'), ('tweeter', 'string')]\n)\n\ngf.list_properties()\n\n\nids            (vertex)  (type: string)\nretweeter      (edge)    (type: string)\ntweeter        (edge)    (type: string)\nweight         (edge)    (type: int32_t)"
  },
  {
    "objectID": "src/analysis.html#data-preparation",
    "href": "src/analysis.html#data-preparation",
    "title": "Twitter Network Analysis",
    "section": "",
    "text": "First, let’s define some helper functions to be used later.\n\n\nCode\nimport json\nimport re\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_bar(x, y, title=None, xtick=None, rotation=0, xlabel=None, ylabel=None, width=0.8, horizontal=False):\n    if horizontal:\n        plt.barh(y=x, width=y, height=width)\n        xlabel, ylabel = ylabel, xlabel\n        plt.gca().invert_yaxis()\n    else:\n        plt.bar(x=x, height=y, width=width)\n    plt.xticks(xtick, rotation=rotation)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.title(title)\n\ndef extract_missing_usernames(df, username_column):\n    pattern = r\"RT @([A-Za-z0-9_]+):\"\n    usernames = []\n    for index, row in df.iterrows():\n        match = re.search(pattern, row[\"retweet_text\"])\n        if match:\n            usernames.append(match.group(1))\n        else:\n            usernames.append(row[username_column])\n    df[username_column] = usernames\n\n    return df\n\n\ndef get_time_range(df):\n    \"\"\"Get the time range of the DataFrame\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame to be queried\n    date_lang : str, optional\n        Language of the date, by default \"de_DE\"\n\n    Returns\n    -------\n    str\n        Start date of the DataFrame\n    str\n        End date of the DataFrame\n    \"\"\"\n    df.retweet_created_at = df.retweet_created_at.astype(\"datetime64[ns, UTC]\")\n    start_date = df.retweet_created_at.min().strftime(\"%B %e, %Y\")\n    end_date = df.retweet_created_at.max().strftime(\"%B %e, %Y\")\n\n    return start_date, end_date\n\n\ndef get_largest_values(df, col_name, n):\n    \"\"\"Get the n largest values of a column in a DataFrame\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame to be queried\n    col_name : str\n        Name of the column to be queried\n    n : int\n        Number of largest values to be returned (i.e. number of rows)\n\n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame with the n largest values of the column\n    \"\"\"\n    top = df.sort_values(col_name, ascending=False).head(n)\n\n    # put col_name as first column\n    cols = top.columns.tolist()\n    cols = cols[-1:] + cols[:-1]\n    top = top[cols]\n\n    return top\n\n\ndef get_top_users(df, df_authors, column_name, n):\n    \"\"\"Get the top n users with their profiles based on a column in a DataFrame\n    Values of the column are standardized so that the largest value is 1.0\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame to be queried\n    df_authors : pandas.DataFrame\n        DataFrame with the usernames and names of authors\n    column_name : str\n        Name of the column to be queried\n    n : int\n        Number of largest values to be returned (i.e. number of rows)\n\n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame with the n largest values of the column\n    \"\"\"\n    column_name_std = column_name + \" (normalised)\"\n    df[column_name_std] = df[column_name] / max(df[column_name])\n    df = get_largest_values(df, column_name, n)\n    df = add_profile_url(df, \"username\")\n    df = pd.merge(df, df_authors, on=\"username\", how=\"left\")\n    df = df.round(5)\n    df.index = np.arange(1, len(df) + 1)\n    df = df[[column_name, column_name_std, \"username\", \"name\", \"profile_url\"]]\n    return df\n\n\ndef get_authors_name(df):\n    \"\"\"Get the usernames and names of retweet authors and tweet authors\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame to be queried\n\n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame with the usernames and names of retweet authors and tweet authors\n    \"\"\"\n    retweet_authors = df[[\"retweet_author_username\", \"retweet_author_name\"]].copy()\n    retweet_authors.rename(\n        columns={\"retweet_author_username\": \"username\", \"retweet_author_name\": \"name\"},\n        inplace=True,\n    )\n\n    tweet_authors = df[[\"tweet_author_username\", \"tweet_author_name\"]].copy()\n    tweet_authors.rename(\n        columns={\"tweet_author_username\": \"username\", \"tweet_author_name\": \"name\"},\n        inplace=True,\n    )\n\n    authors = pd.concat([retweet_authors, tweet_authors])\n    authors = authors.drop_duplicates(subset=[\"username\"], keep=\"last\").reset_index(\n        drop=True\n    )\n\n    return authors\n\n\ndef add_profile_url(df, username_col):\n    df[\"profile_url\"] = \"https://twitter.com/\" + df[username_col]\n\n    return df\n\n\nNow let’s prepare the dataset and print some relevant information about it.\n\n\nCode\n# load and clean dataset\ndf = pd.read_parquet(\"data/raw/all_tweets_lehrkraeftebildung.parquet\")\ndf.replace([\"NaN\", \"nan\", \"None\", \"\"], np.NaN, inplace=True)\ndf = extract_missing_usernames(df, \"tweet_author_username\")\n\n# get information about the retweets\nstart_date, end_date = get_time_range(df)\nsearch_words = \"(Lehrkräftebildung OR Lehrerbildung OR Lehrkräfte OR Lehrkräftefortbildung OR Seiteneinstieg OR Quereinstieg OR Lehramt)\"\nquery_conds = \"(is:retweet OR is:quote) lang:de\"\n\n# drop retweets with missing usernames\nold_df_len = df.shape[0]\ntry:\n    missing_usernames = df.tweet_author_username.isnull().value_counts()[True]\nexcept KeyError:\n    missing_usernames = 0\ndf = df.dropna(subset=[\"tweet_author_username\"])\n\n\n# Print info about dataset\nprint(f\"Number of total retweets in this dataset: \\n{old_df_len}\")\nprint(f\"\\nTime range of the retweets:\\n{start_date} - {end_date}\")\nprint(f\"\\nKeywords* used to collect the retweets:\\n{search_words}\")\nprint(f\"\\nQuery conditions used to collect the retweets:\\n{query_conds}\")\nprint(f\"\\nNumber of retweets with missing usernames for the original tweeter: {missing_usernames}\\nThese are being dropped from the analysis. New total of retweets: {len(df)}\")\n\n\nNumber of total retweets in this dataset: \n11027\n\nTime range of the retweets:\nFebruary 23, 2023 - April  1, 2023\n\nKeywords* used to collect the retweets:\n(Lehrkräftebildung OR Lehrerbildung OR Lehrkräfte OR Lehrkräftefortbildung OR Seiteneinstieg OR Quereinstieg OR Lehramt)\n\nQuery conditions used to collect the retweets:\n(is:retweet OR is:quote) lang:de\n\nNumber of retweets with missing usernames for the original tweeter: 199\nThese are being dropped from the analysis. New total of retweets: 10828\n\n\nNow let’s prepare the graph.\n\n\nCode\nimport graph_tool.all as gt\n\n# prepare dataset for graph\ndf_ = df.copy()\ndf_[\"weight\"] = df_.groupby(['retweet_author_username', 'tweet_author_username']).transform('size')\n\n# choose columns to keep\ncolumns = ['retweet_author_username',\n    'tweet_author_username',\n    \"weight\",\n    ]\n\ndf_ = df_[columns].drop_duplicates(subset=['retweet_author_username', 'tweet_author_username'])\n\n# create list of edges\ng_list = [(r, t, w, r, t) for r, t, w in zip(df_.retweet_author_username, df_.tweet_author_username, df_.weight)]\n\n# create graph\ngf = gt.Graph(\n    g_list,\n    hashed=True,\n    eprops=[('weight', 'int'), ('retweeter', 'string'), ('tweeter', 'string')]\n)\n\ngf.list_properties()\n\n\nids            (vertex)  (type: string)\nretweeter      (edge)    (type: string)\ntweeter        (edge)    (type: string)\nweight         (edge)    (type: int32_t)"
  },
  {
    "objectID": "src/analysis.html#visualising-the-network",
    "href": "src/analysis.html#visualising-the-network",
    "title": "Twitter Network Analysis",
    "section": "Visualising the network",
    "text": "Visualising the network\n\nThe network as a whole\n\n\nCode\ngt.graph_draw(gf)\n;\n\n\n\n\n\n''\n\n\n\n\nThe largest component of the network\nIf we look at the network as a whole, we can see that there are many isolated nodes on its periphery. If we remove those, this is what we get.\n\n\nCode\n# subgraph of largest component\ng = gt.extract_largest_component(gf, directed=False)\ng = gt.Graph(g, prune=True)\ngt.graph_draw(g)\n;\n\n\n\n\n\n''\n\n\n\n\nDifferent layouts\n\nFruchterman-Reingold layout\n\n\nCode\ngt.graph_draw(g, pos=gt.fruchterman_reingold_layout(g, weight=None, a=None, r=1.0, scale=None, circular=False, grid=True, t_range=None, n_iter=10, pos=None))\n;\n\n\n\n\n\n''\n\n\nComment: super expensive, takes a long time to compute even with 10 iterations (that’s why it looks so bad)\n\n\nSfDP layout\n\n\nCode\ngt.graph_draw(g, pos=gt.sfdp_layout(g, vweight=None, eweight=g.ep[\"weight\"], pin=None, groups=None, C=0.2, K=None, p=2.0, theta=0.6, max_level=15, r=1.0, gamma=0.3, mu=2.0, kappa=1.0, rmap=None, R=1, init_step=None, cooling_step=0.95, adaptive_cooling=True, epsilon=0.01, max_iter=0, pos=None, multilevel=None, coarse_method='hybrid', mivs_thres=0.9, ec_thres=0.75, weighted_coarse=False, verbose=False))\n;\n\n\n\n\n\n''\n\n\n\n\nChoose a layout and save positions\n\npos = gt.sfdp_layout(g, eweight=g.edge_properties[\"weight\"])\n\n# create mapping from usernames to vertex ids\nid_usernames = {v: g.vp.ids[v] for v in range(len(list(g.vp.ids)))}"
  },
  {
    "objectID": "src/analysis.html#statistics-of-the-subnetwork",
    "href": "src/analysis.html#statistics-of-the-subnetwork",
    "title": "Twitter Network Analysis",
    "section": "Statistics of the (sub)network",
    "text": "Statistics of the (sub)network\n\nDegree distribution\n\n\nCode\n# in & out degree distribution\n\nin_hist = gt.vertex_hist(g, \"in\")\nout_hist = gt.vertex_hist(g, \"out\")\n\ny = in_hist[0]\nerr = np.sqrt(in_hist[0])\nplt.errorbar(in_hist[1][:-1], in_hist[0], fmt=\"o\", yerr=err,\n        label=\"in\")\n\ny = out_hist[0]\nerr = np.sqrt(out_hist[0])\nplt.errorbar(out_hist[1][:-1], out_hist[0], fmt=\"o\", yerr=err,\n        label=\"out\")\n\nplt.yscale(\"log\")\nplt.xscale(\"log\")\n\nplt.xlabel(\"$k$\")\nplt.ylabel(\"$NP(k_{in})$\")\n\nplt.tight_layout()\nplt.legend()\n;\n\n\n''\n\n\n\n\n\n\n\nWeight distribution\nWeights indicate how many links there are between two nodes. We can see that most nodes have a weight of 1, which means that they have only one link between them.\n\n\nCode\nhist, bins = gt.edge_hist(g, g.ep[\"weight\"], bins=[0, 1], float_count=False)\nbins = bins[:-1]\n\nprint(\"Number of edges with weight:\")\n[print(f\"{b}: {hist[b]}\") for b in range(1, len(bins))]\n\nplot_bar(x=bins, y=hist,\n    title=None,\n    xtick=None, rotation=90,\n    xlabel=None, ylabel=None)\n;\n\n\nNumber of edges with weight:\n1: 7798\n2: 800\n3: 107\n4: 38\n5: 17\n6: 11\n7: 4\n8: 2\n9: 1\n10: 4\n11: 1\n12: 1\n13: 1\n14: 1\n15: 0\n16: 0\n17: 1\n18: 0\n19: 0\n20: 0\n21: 0\n22: 0\n23: 0\n24: 0\n25: 0\n26: 0\n27: 0\n28: 0\n29: 0\n30: 0\n31: 0\n32: 0\n33: 0\n34: 0\n35: 0\n36: 1\n\n\n''\n\n\n\n\n\n\n\nAverage path length\n\n\nCode\naverage_path_length_in = gt.vertex_average(g, \"in\")\naverage_path_length_out = gt.vertex_average(g, \"out\")\naverage_path_length_total = (gt.vertex_average(g, \"total\"))\n\nprint(f\"Average path length of the network (total): {average_path_length_total[0]} with std: {average_path_length_total[1]}\")\nprint(f\"Average path length of the network (in): {average_path_length_in[0]} with std: {average_path_length_in[1]}\")\nprint(f\"Average path length of the network (out): {average_path_length_out[0]} with std: {average_path_length_out[1]}\")\n\n# plot\n# plt.bar([\"in\", \"out\", \"total\"], [average_path_length_in[0], average_path_length_out[0], average_path_length_total[0]])\n# plt.ylabel(\"Average path length\")\n# plt.title(\"Average path length of the network\")\n\n\nAverage path length of the network (total): 2.9509738079247816 with std: 0.1659641564735749\nAverage path length of the network (in): 1.4754869039623908 with std: 0.15507219510386128\nAverage path length of the network (out): 1.4754869039623908 with std: 0.05750973570288464\n\n\nComment: why are in and out the same? The network is directed, so the in and out path lengths should be different, no?\n\n\nShortest distance distribution\nPlot the shortest-distance histogram for each vertex pair in the graph.\n\n\nCode\nhist, bins = gt.distance_histogram(g, weight=None, bins=[0, 1], samples=None, float_count=False)\nbins = bins[:-1]\n\n[print(f\"{b}: {hist[b]}\") for b in range(1, len(bins))]\n\nplot_bar(x=bins, y=hist, width=1,\n    title=\"Shortest distance distribution\",\n    xtick=bins, rotation=90,\n    xlabel=\"Distance\", ylabel=\"Count\")\n;\n\n\n1: 8743\n2: 13474\n3: 13850\n4: 14242\n5: 16132\n6: 13068\n7: 9925\n8: 7192\n9: 6137\n10: 6683\n11: 6783\n12: 5910\n13: 9495\n14: 11039\n15: 8258\n16: 4897\n17: 2371\n18: 850\n19: 148\n20: 13\n\n\n''"
  },
  {
    "objectID": "src/analysis.html#measures-of-centrality",
    "href": "src/analysis.html#measures-of-centrality",
    "title": "Twitter Network Analysis",
    "section": "Measures of centrality",
    "text": "Measures of centrality\n\nDegree centrality\nDegree centrality represents the number of connections a node has.\n\nIn-degree centrality\nIn-degree centrality represents the number of connections going into a node. In the case of retweets, in-degree centrality will indicate that a user is getting a large number of retweets.\n\n\nCode\nin_deg = g.get_in_degrees(g.get_vertices())\n\ndic = {b: in_deg[b] for b in range(1, len(in_deg))}\nvalues_sum = sum(dic.values())\ndic = {key: val for key, val in (sorted(dic.items(), key=lambda x: x[1], reverse=True)[:20])}\n\n# map names to ids\ndic = {id_usernames[k]: v for k, v in dic.items()}\n\n# prepare table\nscore_name = \"In-Degrees\"\ndf_ = pd.DataFrame(dic.items(), columns=[\"Username\", score_name])\ndf_[\"(normalised)\"] = (df_[score_name] / df_[score_name].max()).round(2)\ndf_[\"(share)\"] = ((df_[score_name] / values_sum)*100).round(2)\ndf_.index = np.arange(1, len(df_) + 1)\n\nprint(df_)\n\nplot_bar(x=list(dic.keys()), y=dic.values(), horizontal=True,\n    title=f\"Top 20 users by {score_name}\",\n    xtick=None, rotation=90,\n    xlabel=\"Username\", ylabel=\"Count\");\n;\n\n\n           Username  In-Degrees  (normalised)  (share)\n1     leseerlaubnis         411          1.00     4.68\n2      BentFreiwald         352          0.86     4.01\n3   Georg_Pazderski         319          0.78     3.63\n4         blume_bob         263          0.64     2.99\n5       juliansfrth         250          0.61     2.84\n6    lueckenbildung         229          0.56     2.61\n7     BildungSicher         207          0.50     2.36\n8        Deu_Kurier         198          0.48     2.25\n9      DurdenGaming         171          0.42     1.95\n10       WDRaktuell         148          0.36     1.68\n11            dm_ms         110          0.27     1.25\n12    News4teachers          84          0.20     0.96\n13  SozialarbeitNRW          83          0.20     0.94\n14        Lilotanzt          73          0.18     0.83\n15      ehrenlehrer          71          0.17     0.81\n16       zeitonline          71          0.17     0.81\n17      rosenbusch_          68          0.17     0.77\n18  fingurplaustert          68          0.17     0.77\n19        elemob_de          66          0.16     0.75\n20   OliverSKBerlin          63          0.15     0.72\n\n\n''\n\n\n\n\n\n\n\nOut-degree centrality\nOut-degree centrality represents the number of connections going out of a node. In the case of retweets, out-degree centrality will indicate that a user is retweeting a lot.\n\n\nCode\nout_deg = g.get_out_degrees(g.get_vertices())\n\ndic = {b: out_deg[b] for b in range(1, len(out_deg))}\nvalues_sum = sum(dic.values())\ndic = {key: val for key, val in (sorted(dic.items(), key=lambda x: x[1], reverse=True)[:20])}\n\n# map names to ids\ndic = {id_usernames[k]: v for k, v in dic.items()}\n\n# prepare table\nscore_name = \"Out-Degrees\"\ndf_ = pd.DataFrame(dic.items(), columns=[\"Username\", score_name])\ndf_[\"(normalised)\"] = (df_[score_name] / df_[score_name].max()).round(2)\ndf_[\"(share)\"] = ((df_[score_name] / values_sum)*100).round(2)\ndf_.index = np.arange(1, len(df_) + 1)\n\nplot_bar(x=list(dic.keys()), y=dic.values(), horizontal=True,\n    title=f\"Top 20 users by {score_name}\",\n    xtick=None, rotation=90,\n    xlabel=\"Username\", ylabel=\"Count\");\n;\n\nprint(df_)\n\n\n           Username  Out-Degrees  (normalised)  (share)\n1     Bot_TwLehrerZ          319          1.00     3.63\n2          Doriiien           31          0.10     0.35\n3            Ju1820           25          0.08     0.28\n4          goesicke           23          0.07     0.26\n5      Mandala81966           22          0.07     0.25\n6          sanjosko           21          0.07     0.24\n7   Arsenio57509279           20          0.06     0.23\n8          eduBWbot           19          0.06     0.22\n9          lothenon           18          0.06     0.20\n10           _Wir_4           18          0.06     0.20\n11          lgoshen           16          0.05     0.18\n12   julian07646263           16          0.05     0.18\n13         printi71           16          0.05     0.18\n14     miaumiau1975           16          0.05     0.18\n15          seni_bl           15          0.05     0.17\n16    KinderJournal           14          0.04     0.16\n17  Soultiger091111           14          0.04     0.16\n18    SvenDudkowiak           14          0.04     0.16\n19        die_socke           14          0.04     0.16\n20      nicole37911           14          0.04     0.16\n\n\n\n\n\nComment: seems like a bot is retweeting a lot more than any other user.\n\n\n\nBetweenness centrality\nBetweeness centrality represents the number of all ‘shortest paths’ between nodes that pass through a specific node. In other words, it counts how often that node is part of a short connection. In the case of retweets, it measures the extent to which a user connects different communities of users.\n\nUseful when there is flux / information flow within network\nHelps predict and locate vulnerability of a network (i.e. if you remove a node with high betweeness)\nProblem: computationally expensive (as you need to calculate all shortest paths)\n\n\n\nCode\nvp, ep = gt.betweenness(g)\n\ngt.graph_draw(g, pos=pos, vertex_fill_color=vp,\n              vertex_size=gt.prop_to_size(vp, mi=1, ma=10),\n              edge_pen_width=gt.prop_to_size(ep, mi=0.5, ma=5),\n              vcmap=plt.cm.autumn,\n              vorder=vp)\n;\n\n\n\n\n\n''\n\n\nCalculate top users by betweenness centrality\n\n\nCode\ndic = {i: vp.a[i] for i in range(g.num_vertices())}\ndic = {key: val for key, val in (sorted(dic.items(), key=lambda x: x[1], reverse=True)[:20])}\n\n# map names to ids\ndic = {id_usernames[k]: v for k, v in dic.items()}\n\n# prepare table\nscore_name = \"Betweenness Score\"\ndf_ = pd.DataFrame(dic.items(), columns=[\"Username\", score_name])\ndf_[\"(normalised)\"] = (df_[score_name] / df_[score_name].max()).round(2)\ndf_ = df_.round(5)\ndf_.index = np.arange(1, len(df_) + 1)\n\nprint(df_)\n\nplot_bar(x=list(dic.keys()), y=dic.values(), horizontal=True,\n    title=f\"Top 20 users by {score_name}\",\n    xtick=None, rotation=90,\n    xlabel=\"Username\", ylabel=\"Count\")\n;\n\n\n           Username  Betweenness Score  (normalised)\n1     leseerlaubnis            0.00192          1.00\n2      BentsCristin            0.00184          0.96\n3          SicherEU            0.00180          0.94\n4    EberhardSchlie            0.00165          0.86\n5     Haselmaus2010            0.00152          0.79\n6         Diggi_tal            0.00152          0.79\n7      EINSBERGBLOG            0.00151          0.79\n8       hav_hendrik            0.00126          0.66\n9   JSchmitzLeipzig            0.00122          0.64\n10      Herr_Nuxoll            0.00119          0.62\n11            ciffi            0.00115          0.60\n12    risikogruppen            0.00085          0.44\n13     sdw_Stiftung            0.00081          0.42\n14   HSE_Heidelberg            0.00080          0.42\n15  zumWeitertragen            0.00072          0.37\n16           SenBJF            0.00068          0.35\n17    PartnerSchule            0.00059          0.31\n18      DianaKnodel            0.00053          0.27\n19         Woe_Real            0.00050          0.26\n20       BerlinSoul            0.00046          0.24\n\n\n''\n\n\n\n\n\nCalculate top edges by betweenness centrality\n\n\nCode\ndic = {i: ep.a[i] for i in range(g.num_edges())}\ndic = {key: val for key, val in (sorted(dic.items(), key=lambda x: x[1], reverse=True)[:20])}\n\n# get names of tweeters and retweeters\nlis = [(list(g.ep.tweeter)[k], list(g.ep.retweeter)[k], v) for k, v in dic.items()]\n\n# prepare table\nscore_name = \"Betweenness Score\"\ndf_ = pd.DataFrame(lis, columns=[\"Tweeter\", \"Retweeter\", score_name])\ndf_[\"(normalised)\"] = (df_[score_name] / df_[score_name].max()).round(2)\ndf_ = df_.round(5)\ndf_.index = np.arange(1, len(df_) + 1)\n\nprint(df_)\n\nplot_bar(x=df_.index, y=df_[score_name],\n    title=f\"Top 20 edges by {score_name}\",\n    xtick=df_.index, rotation=90,\n    xlabel=\"Edges\", ylabel=\"Count\");\n\n\n            Tweeter        Retweeter  Betweenness Score  (normalised)\n1          SicherEU     BentsCristin            0.00172          1.00\n2      BentsCristin   EberhardSchlie            0.00165          0.96\n3    EberhardSchlie    leseerlaubnis            0.00157          0.92\n4     Haselmaus2010        Diggi_tal            0.00152          0.88\n5      EINSBERGBLOG    Haselmaus2010            0.00149          0.87\n6       Herr_Nuxoll      hav_hendrik            0.00121          0.70\n7       hav_hendrik  JSchmitzLeipzig            0.00118          0.69\n8             ciffi      Herr_Nuxoll            0.00115          0.67\n9   JSchmitzLeipzig     EINSBERGBLOG            0.00097          0.57\n10        Diggi_tal    risikogruppen            0.00081          0.47\n11   HSE_Heidelberg     sdw_Stiftung            0.00081          0.47\n12    risikogruppen         SicherEU            0.00075          0.44\n13  zumWeitertragen         SicherEU            0.00072          0.42\n14        Diggi_tal  zumWeitertragen            0.00072          0.42\n15    PartnerSchule           SenBJF            0.00061          0.35\n16     sdw_Stiftung    PartnerSchule            0.00059          0.34\n17         Woe_Real            ciffi            0.00052          0.31\n18           SenBJF     EINSBERGBLOG            0.00052          0.30\n19  StaackSebastian  dreadlock_dread            0.00039          0.23\n20       BerlinSoul   regina_kittler            0.00038          0.22\n\n\n\n\n\n\n\nEigenvalue centrality\nSum of all the eigenvector centralities of its neighbours. My importance is dependent on the importance of the nodes which I am connected to. It is recursive: so my centrality depends on the centrality of my neighbours, which depends on the centrality of their neighbours, and so on. Takes into account the whole network.\n\n\nCode\ng.vertex_properties[\"eigenvector\"] = g.new_vertex_property(\"double\")\n\nmax_eigenvalue, _ = gt.eigenvector(g, weight=g.ep[\"weight\"], vprop=g.vp[\"eigenvector\"], epsilon=1e-6, max_iter=1000)\n\nprint(max_eigenvalue)\n\n\n0.0\n\n\nComment: eigenvalues are returning all 0s. Why?\n\n\nKatz centrality\nKatz: makes use of the adjacency matrix. Similar to eigenvector, but includes an alpha factor that gradually reduces the influence of further nodes. So the difference is that you can set (through alpha) how much influence local neighbours have in comparison to far away neighbours.\n\n\nCode\nimport seaborn as sns\n\nalphas = [0.1, 0.01, 0.001]\n\ndf_merged = pd.DataFrame()\n\nfor i, alpha in enumerate(alphas):\n\n    vp_name = f\"katz_{alpha}\"\n    g.vertex_properties[vp_name] = g.new_vertex_property(\"double\")\n\n    gt.katz(g, alpha=alpha, beta=None, weight=g.ep[\"weight\"], vprop=g.vp[vp_name], epsilon=1e-06, max_iter=None, norm=True)\n\n    dic = {i: g.vp[vp_name][i] for i in range(len(list(g.vp[vp_name])))}\n    #dic = {key: val for key, val in (sorted(dic.items(), key=lambda x: x[1], reverse=True))}\n    dic = {id_usernames[k]: v for k, v in dic.items()}\n\n    # prepare table\n    score_name = f\"Katz Score (a={alpha})\"\n    df_ = pd.DataFrame(dic.items(), columns=[\"Username\", score_name])\n    #df_[\"(normalised)\"] = (df_[score_name] / df_[score_name].max()).round(2)\n    df_ = df_.round(5)\n    df_.index = np.arange(1, len(df_) + 1)\n\n    if i == 0:\n        df_merged = df_\n    else:\n        # df_ = df_[[\"Username\", score_name]]\n        df_merged = df_merged.merge(df_, on=\"Username\", how=\"left\", suffixes=(\"\", f\"_{alpha}\"))\n\ndf_top20 = df_merged.sort_values(by=df_merged.columns[1], ascending=False).head(20)\n\nprint(df_top20)\n\n\np = sns.catplot(\n    data=df_top20.melt(id_vars='Username'), kind=\"bar\",\n    y=\"Username\", x=\"value\", hue=\"variable\",\n    legend_out=False, aspect=1.5\n)\nplt.title(\"Katz centrality for different alpha values\")\n;\n\n\n             Username  Katz Score (a=0.1)  Katz Score (a=0.01)   \n101     leseerlaubnis             0.40590              0.08876  \\\n361          SicherEU             0.25172              0.02081   \n1071    BildungSicher             0.23865              0.04777   \n619         blume_bob             0.22327              0.05060   \n1615     BentFreiwald             0.21547              0.05723   \n1364  Georg_Pazderski             0.21178              0.05875   \n1      lueckenbildung             0.18936              0.04463   \n3998      juliansfrth             0.17636              0.04936   \n1537    onlineschulen             0.16727              0.01964   \n124     News4teachers             0.12658              0.02507   \n3620       WDRaktuell             0.11768              0.03572   \n282        Deu_Kurier             0.11715              0.03757   \n4343   ARD_Presseclub             0.11685              0.02296   \n4726     DurdenGaming             0.10809              0.03426   \n272             dm_ms             0.10724              0.02739   \n30      KinderJournal             0.10326              0.01659   \n488   SozialarbeitNRW             0.08621              0.02513   \n3072          DBmS_de             0.08457              0.01511   \n1428    risikogruppen             0.08073              0.01461   \n3254  zumWeitertragen             0.07941              0.01361   \n\n      Katz Score (a=0.001)  \n101                0.02063  \n361                0.01369  \n1071               0.01643  \n619                0.01677  \n1615               0.01749  \n1364               0.01767  \n1                  0.01615  \n3998               0.01670  \n1537               0.01361  \n124                0.01418  \n3620               0.01529  \n282                0.01549  \n4343               0.01391  \n4726               0.01515  \n272                0.01444  \n30                 0.01331  \n488                0.01420  \n3072               0.01317  \n1428               0.01312  \n3254               0.01301  \n\n\n''\n\n\n\n\n\n\n\nPagerank\nSimilar to eigenvector. For a random walker on a network, what is the expected probability of it being on that node, plus the probability that it jumps to somewhere completely different. The smaller the alpha, the more it jumps around. The higher the number that walker passes through a node, the higher the centrality of that node.\nAdding some noise makse sure results does not point you to the same nodes every time.\n\n\nCode\ng.vertex_properties[\"pagerank\"] = g.new_vertex_property(\"double\")\n\ngt.pagerank(g, damping=0.85, pers=None, weight=g.ep[\"weight\"], prop=g.vp[\"pagerank\"], epsilon=1e-06, max_iter=None, ret_iter=False)\n\ndic = {i: g.vp[\"pagerank\"][i] for i in range(len(list(g.vp[\"pagerank\"])))}\ndic = {key: val for key, val in (sorted(dic.items(), key=lambda x: x[1], reverse=True)[:20])}\n\n# map names to ids\ndic = {id_usernames[k]: v for k, v in dic.items()}\n\n# prepare table\nscore_name = \"Pagerank Score\"\ndf_ = pd.DataFrame(dic.items(), columns=[\"Username\", score_name])\ndf_[\"(normalised)\"] = (df_[score_name] / df_[score_name].max()).round(2)\ndf_ = df_.round(5)\ndf_.index = np.arange(1, len(df_) + 1)\n\nprint(df_)\n\nplot_bar(x=dic.keys(), y=dic.values(),\n    title=f\"Top 20 users by {score_name}\",\n    xtick=None, rotation=90,\n    xlabel=\"Username\", ylabel=\"Count\")\n;\n\n\n           Username  Pagerank Score  (normalised)\n1    ARD_Presseclub         0.03629          1.00\n2     BildungSicher         0.02613          0.72\n3       rosenbusch_         0.02271          0.63\n4      BentFreiwald         0.01978          0.55\n5     leseerlaubnis         0.01959          0.54\n6   Georg_Pazderski         0.01578          0.43\n7       juliansfrth         0.01498          0.41\n8         blume_bob         0.01307          0.36\n9     AnthroBlogger         0.01297          0.36\n10   lueckenbildung         0.01285          0.35\n11   ernsterjuenger         0.01099          0.30\n12       Deu_Kurier         0.00939          0.26\n13     DurdenGaming         0.00837          0.23\n14    Anna_Lena2022         0.00764          0.21\n15    BildgSicherBW         0.00752          0.21\n16         JMWiarda         0.00720          0.20\n17  AladinMafaalani         0.00631          0.17\n18       WDRaktuell         0.00614          0.17\n19       RSandra266         0.00609          0.17\n20          LHaasis         0.00564          0.16\n\n\n''"
  },
  {
    "objectID": "src/analysis.html#other-stuff-i.e.-i-dont-know-what-im-doing",
    "href": "src/analysis.html#other-stuff-i.e.-i-dont-know-what-im-doing",
    "title": "Twitter Network Analysis",
    "section": "Other stuff (i.e. I don’t know what I’m doing)",
    "text": "Other stuff (i.e. I don’t know what I’m doing)\n\nLocal clustering\n\n\nCode\ng.vertex_properties[\"local_clustering\"] = g.new_vertex_property(\"double\")\n\ngt.local_clustering(g, prop=g.vp[\"local_clustering\"], undirected=False)\n\n\n&lt;VertexPropertyMap object with value type 'double', for Graph 0x7fa5d5800a30, at 0x7fa56cd17100&gt;\n\n\nComments: how do I use this?\n\n\nPseudo-diameter\n\n\nCode\ngt.pseudo_diameter(g,  source=None, weights=g.ep[\"weight\"])\n\n\n(1.0,\n (&lt;Vertex object with index '0' at 0x7fa56cfb45c0&gt;,\n  &lt;Vertex object with index '453' at 0x7fa56cfb4dc0&gt;))\n\n\n\n\nRadial layout\n\n\nCode\nroot = g.vertex(101) # username: leseerlaubnis\n\ngt.graph_draw(g, pos=gt.radial_tree_layout(g, root, rel_order=None, rel_order_leaf=False, weighted=False, node_weight=None, r=1.0))\n\n\n\n\n\n&lt;VertexPropertyMap object with value type 'vector&lt;double&gt;', for Graph 0x7fa5d5800a30, at 0x7fa56c9a74c0&gt;\n\n\nComment: what does that show, exactly?\n\n\nBlockmodel\n\n\nCode\nstate = gt.minimize_blockmodel_dl(g)\n\nstate.draw(vertex_shape=state.get_blocks())\n\n\n\n\n\n&lt;VertexPropertyMap object with value type 'vector&lt;double&gt;', for Graph 0x7fa5d5800a30, at 0x7fa56c9936d0&gt;\n\n\n\n\nCode\nstate2 = gt.minimize_nested_blockmodel_dl(g)\n\nstate2.draw()\n\n\n\n\n\n(&lt;VertexPropertyMap object with value type 'vector&lt;double&gt;', for Graph 0x7fa5d5800a30, at 0x7fa56c9d2470&gt;,\n &lt;GraphView object, directed, with 5973 vertices and 5972 edges, edges filtered by (&lt;EdgePropertyMap object with value type 'bool', for Graph 0x7fa56c9d3610, at 0x7fa56cec8040&gt;, False), vertices filtered by (&lt;VertexPropertyMap object with value type 'bool', for Graph 0x7fa56c9d3610, at 0x7fa63ea82cb0&gt;, False), at 0x7fa56c9d3610&gt;,\n &lt;VertexPropertyMap object with value type 'vector&lt;double&gt;', for Graph 0x7fa56c9d3610, at 0x7fa56cec8640&gt;)\n\n\nComment: What is going on here??\n\n\nCode\nstate = gt.minimize_nested_blockmodel_dl(g)\n\nt = gt.get_hierarchy_tree(state)[0]\n\ntpos = pos = gt.radial_tree_layout(t, t.vertex(t.num_vertices() - 1, use_index=False), weighted=True)\n\ncts = gt.get_hierarchy_control_points(g, t, tpos)\n\npos = g.own_property(tpos)\n\nb = state.levels[0].b\n\nshape = b.copy()\n\nshape.a %= 14\n\ngt.graph_draw(g, pos=pos, vertex_fill_color=b, vertex_shape=shape, edge_control_points=cts,\nedge_color=[0, 0, 0, 0.3], vertex_anchor=0)\n\n\n\n\n\n&lt;VertexPropertyMap object with value type 'vector&lt;double&gt;', for Graph 0x7fa5d5800a30, at 0x7fa573446890&gt;\n\n\n\n\nEdge percolation\n\n\nCode\nedges = sorted([(e.source(), e.target()) for e in g.edges()],\n               key=lambda e: e[0].in_degree() * e[1].in_degree())\n\nsizes, comp = gt.edge_percolation(g, edges)\n\nnp.random.shuffle(edges)\n\nsizes2, comp = gt.edge_percolation(g, edges)\n\nplt.plot(sizes, label=\"Targeted\")\n\nplt.plot(sizes2, label=\"Random\")\n\nplt.legend()\n\n\n&lt;matplotlib.legend.Legend at 0x7fa56ceca350&gt;\n\n\n\n\n\n\n\nK-core decpomposition\n\n\nCode\nkcore = gt.kcore_decomposition(g)\ngt.graph_draw(g, vertex_fill_color=kcore, vertex_text=kcore)\n\n\n\n\n\n&lt;VertexPropertyMap object with value type 'vector&lt;double&gt;', for Graph 0x7fa5d5800a30, at 0x7fa56ca58910&gt;"
  },
  {
    "objectID": "src/analysis.html#comments-issues",
    "href": "src/analysis.html#comments-issues",
    "title": "Twitter Network Analysis",
    "section": "COMMENTS / ISSUES",
    "text": "COMMENTS / ISSUES"
  }
]