[
  {
    "objectID": "src/analysis.html",
    "href": "src/analysis.html",
    "title": "Twitter Network Analysis",
    "section": "",
    "text": "First, let’s define some helper functions to be used later.\n\n\nCode\nimport json\nimport re\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_bar(x, y, title=None, xtick=None, rotation=0, xlabel=None, ylabel=None, width=0.8, horizontal=False):\n    if horizontal:\n        plt.barh(y=x, width=y, height=width)\n        xlabel, ylabel = ylabel, xlabel\n        plt.gca().invert_yaxis()\n    else:\n        plt.bar(x=x, height=y, width=width)\n    plt.xticks(xtick, rotation=rotation)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.title(title)\n\ndef extract_missing_usernames(df, username_column):\n    pattern = r\"RT @([A-Za-z0-9_]+):\"\n    usernames = []\n    for index, row in df.iterrows():\n        match = re.search(pattern, row[\"retweet_text\"])\n        if match:\n            usernames.append(match.group(1))\n        else:\n            usernames.append(row[username_column])\n    df[username_column] = usernames\n\n    return df\n\n\ndef get_time_range(df):\n    \"\"\"Get the time range of the DataFrame\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame to be queried\n    date_lang : str, optional\n        Language of the date, by default \"de_DE\"\n\n    Returns\n    -------\n    str\n        Start date of the DataFrame\n    str\n        End date of the DataFrame\n    \"\"\"\n    df.retweet_created_at = df.retweet_created_at.astype(\"datetime64[ns, UTC]\")\n    start_date = df.retweet_created_at.min().strftime(\"%B %e, %Y\")\n    end_date = df.retweet_created_at.max().strftime(\"%B %e, %Y\")\n\n    return start_date, end_date\n\n\ndef get_largest_values(df, col_name, n):\n    \"\"\"Get the n largest values of a column in a DataFrame\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame to be queried\n    col_name : str\n        Name of the column to be queried\n    n : int\n        Number of largest values to be returned (i.e. number of rows)\n\n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame with the n largest values of the column\n    \"\"\"\n    top = df.sort_values(col_name, ascending=False).head(n)\n\n    # put col_name as first column\n    cols = top.columns.tolist()\n    cols = cols[-1:] + cols[:-1]\n    top = top[cols]\n\n    return top\n\n\ndef get_top_users(df, df_authors, column_name, n):\n    \"\"\"Get the top n users with their profiles based on a column in a DataFrame\n    Values of the column are standardized so that the largest value is 1.0\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame to be queried\n    df_authors : pandas.DataFrame\n        DataFrame with the usernames and names of authors\n    column_name : str\n        Name of the column to be queried\n    n : int\n        Number of largest values to be returned (i.e. number of rows)\n\n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame with the n largest values of the column\n    \"\"\"\n    column_name_std = column_name + \" (normalised)\"\n    df[column_name_std] = df[column_name] / max(df[column_name])\n    df = get_largest_values(df, column_name, n)\n    df = add_profile_url(df, \"username\")\n    df = pd.merge(df, df_authors, on=\"username\", how=\"left\")\n    df = df.round(5)\n    df.index = np.arange(1, len(df) + 1)\n    df = df[[column_name, column_name_std, \"username\", \"name\", \"profile_url\"]]\n    return df\n\n\ndef get_authors_name(df):\n    \"\"\"Get the usernames and names of retweet authors and tweet authors\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame to be queried\n\n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame with the usernames and names of retweet authors and tweet authors\n    \"\"\"\n    retweet_authors = df[[\"retweet_author_username\", \"retweet_author_name\"]].copy()\n    retweet_authors.rename(\n        columns={\"retweet_author_username\": \"username\", \"retweet_author_name\": \"name\"},\n        inplace=True,\n    )\n\n    tweet_authors = df[[\"tweet_author_username\", \"tweet_author_name\"]].copy()\n    tweet_authors.rename(\n        columns={\"tweet_author_username\": \"username\", \"tweet_author_name\": \"name\"},\n        inplace=True,\n    )\n\n    authors = pd.concat([retweet_authors, tweet_authors])\n    authors = authors.drop_duplicates(subset=[\"username\"], keep=\"last\").reset_index(\n        drop=True\n    )\n\n    return authors\n\n\ndef add_profile_url(df, username_col):\n    df[\"profile_url\"] = \"https://twitter.com/\" + df[username_col]\n\n    return df\n\n\nNow let’s prepare the dataset and print some relevant information about it.\n\n\nCode\n# load and clean dataset\ndf = pd.read_parquet(\"data/raw/all_tweets_lehrkraeftebildung.parquet\")\ndf.replace([\"NaN\", \"nan\", \"None\", \"\"], np.NaN, inplace=True)\ndf = extract_missing_usernames(df, \"tweet_author_username\")\n\n# get information about the retweets\nstart_date, end_date = get_time_range(df)\nsearch_words = \"(Lehrkräftebildung OR Lehrerbildung OR Lehrkräfte OR Lehrkräftefortbildung OR Seiteneinstieg OR Quereinstieg OR Lehramt)\"\nquery_conds = \"(is:retweet OR is:quote) lang:de\"\n\n# drop retweets with missing usernames\nold_df_len = df.shape[0]\ntry:\n    missing_usernames = df.tweet_author_username.isnull().value_counts()[True]\nexcept KeyError:\n    missing_usernames = 0\ndf = df.dropna(subset=[\"tweet_author_username\"])\n\n\n# Print info about dataset\nprint(f\"Number of total retweets in this dataset: \\n{old_df_len}\")\nprint(f\"\\nTime range of the retweets:\\n{start_date} - {end_date}\")\nprint(f\"\\nKeywords* used to collect the retweets:\\n{search_words}\")\nprint(f\"\\nQuery conditions used to collect the retweets:\\n{query_conds}\")\nprint(f\"\\nNumber of retweets with missing usernames for the original tweeter: {missing_usernames}\\nThese are being dropped from the analysis. New total of retweets: {len(df)}\")\n\n\nNumber of total retweets in this dataset: \n16935\n\nTime range of the retweets:\nFebruary 23, 2023 - May  4, 2023\n\nKeywords* used to collect the retweets:\n(Lehrkräftebildung OR Lehrerbildung OR Lehrkräfte OR Lehrkräftefortbildung OR Seiteneinstieg OR Quereinstieg OR Lehramt)\n\nQuery conditions used to collect the retweets:\n(is:retweet OR is:quote) lang:de\n\nNumber of retweets with missing usernames for the original tweeter: 239\nThese are being dropped from the analysis. New total of retweets: 16696\n\n\nNow let’s prepare the graph.\n\n\nCode\nimport graph_tool.all as gt\n\n# prepare dataset for graph\ndf_ = df.copy()\ndf_[\"weight\"] = df_.groupby(['retweet_author_username', 'tweet_author_username']).transform('size')\n\n# choose columns to keep\ncolumns = ['retweet_author_username',\n    'tweet_author_username',\n    \"weight\",\n    ]\n\ndf_ = df_[columns].drop_duplicates(subset=['retweet_author_username', 'tweet_author_username'])\n\n# create list of edges\ng_list = [(r, t, w, r, t) for r, t, w in zip(df_.retweet_author_username, df_.tweet_author_username, df_.weight)]\n\n# create graph\ngf = gt.Graph(\n    g_list,\n    hashed=True,\n    eprops=[('weight', 'int'), ('retweeter', 'string'), ('tweeter', 'string')]\n)\n\ngf.list_properties()\n\n\nids            (vertex)  (type: string)\nretweeter      (edge)    (type: string)\ntweeter        (edge)    (type: string)\nweight         (edge)    (type: int32_t)"
  },
  {
    "objectID": "src/analysis.html#data-preparation",
    "href": "src/analysis.html#data-preparation",
    "title": "Twitter Network Analysis",
    "section": "",
    "text": "First, let’s define some helper functions to be used later.\n\n\nCode\nimport json\nimport re\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_bar(x, y, title=None, xtick=None, rotation=0, xlabel=None, ylabel=None, width=0.8, horizontal=False):\n    if horizontal:\n        plt.barh(y=x, width=y, height=width)\n        xlabel, ylabel = ylabel, xlabel\n        plt.gca().invert_yaxis()\n    else:\n        plt.bar(x=x, height=y, width=width)\n    plt.xticks(xtick, rotation=rotation)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.title(title)\n\ndef extract_missing_usernames(df, username_column):\n    pattern = r\"RT @([A-Za-z0-9_]+):\"\n    usernames = []\n    for index, row in df.iterrows():\n        match = re.search(pattern, row[\"retweet_text\"])\n        if match:\n            usernames.append(match.group(1))\n        else:\n            usernames.append(row[username_column])\n    df[username_column] = usernames\n\n    return df\n\n\ndef get_time_range(df):\n    \"\"\"Get the time range of the DataFrame\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame to be queried\n    date_lang : str, optional\n        Language of the date, by default \"de_DE\"\n\n    Returns\n    -------\n    str\n        Start date of the DataFrame\n    str\n        End date of the DataFrame\n    \"\"\"\n    df.retweet_created_at = df.retweet_created_at.astype(\"datetime64[ns, UTC]\")\n    start_date = df.retweet_created_at.min().strftime(\"%B %e, %Y\")\n    end_date = df.retweet_created_at.max().strftime(\"%B %e, %Y\")\n\n    return start_date, end_date\n\n\ndef get_largest_values(df, col_name, n):\n    \"\"\"Get the n largest values of a column in a DataFrame\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame to be queried\n    col_name : str\n        Name of the column to be queried\n    n : int\n        Number of largest values to be returned (i.e. number of rows)\n\n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame with the n largest values of the column\n    \"\"\"\n    top = df.sort_values(col_name, ascending=False).head(n)\n\n    # put col_name as first column\n    cols = top.columns.tolist()\n    cols = cols[-1:] + cols[:-1]\n    top = top[cols]\n\n    return top\n\n\ndef get_top_users(df, df_authors, column_name, n):\n    \"\"\"Get the top n users with their profiles based on a column in a DataFrame\n    Values of the column are standardized so that the largest value is 1.0\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame to be queried\n    df_authors : pandas.DataFrame\n        DataFrame with the usernames and names of authors\n    column_name : str\n        Name of the column to be queried\n    n : int\n        Number of largest values to be returned (i.e. number of rows)\n\n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame with the n largest values of the column\n    \"\"\"\n    column_name_std = column_name + \" (normalised)\"\n    df[column_name_std] = df[column_name] / max(df[column_name])\n    df = get_largest_values(df, column_name, n)\n    df = add_profile_url(df, \"username\")\n    df = pd.merge(df, df_authors, on=\"username\", how=\"left\")\n    df = df.round(5)\n    df.index = np.arange(1, len(df) + 1)\n    df = df[[column_name, column_name_std, \"username\", \"name\", \"profile_url\"]]\n    return df\n\n\ndef get_authors_name(df):\n    \"\"\"Get the usernames and names of retweet authors and tweet authors\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame to be queried\n\n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame with the usernames and names of retweet authors and tweet authors\n    \"\"\"\n    retweet_authors = df[[\"retweet_author_username\", \"retweet_author_name\"]].copy()\n    retweet_authors.rename(\n        columns={\"retweet_author_username\": \"username\", \"retweet_author_name\": \"name\"},\n        inplace=True,\n    )\n\n    tweet_authors = df[[\"tweet_author_username\", \"tweet_author_name\"]].copy()\n    tweet_authors.rename(\n        columns={\"tweet_author_username\": \"username\", \"tweet_author_name\": \"name\"},\n        inplace=True,\n    )\n\n    authors = pd.concat([retweet_authors, tweet_authors])\n    authors = authors.drop_duplicates(subset=[\"username\"], keep=\"last\").reset_index(\n        drop=True\n    )\n\n    return authors\n\n\ndef add_profile_url(df, username_col):\n    df[\"profile_url\"] = \"https://twitter.com/\" + df[username_col]\n\n    return df\n\n\nNow let’s prepare the dataset and print some relevant information about it.\n\n\nCode\n# load and clean dataset\ndf = pd.read_parquet(\"data/raw/all_tweets_lehrkraeftebildung.parquet\")\ndf.replace([\"NaN\", \"nan\", \"None\", \"\"], np.NaN, inplace=True)\ndf = extract_missing_usernames(df, \"tweet_author_username\")\n\n# get information about the retweets\nstart_date, end_date = get_time_range(df)\nsearch_words = \"(Lehrkräftebildung OR Lehrerbildung OR Lehrkräfte OR Lehrkräftefortbildung OR Seiteneinstieg OR Quereinstieg OR Lehramt)\"\nquery_conds = \"(is:retweet OR is:quote) lang:de\"\n\n# drop retweets with missing usernames\nold_df_len = df.shape[0]\ntry:\n    missing_usernames = df.tweet_author_username.isnull().value_counts()[True]\nexcept KeyError:\n    missing_usernames = 0\ndf = df.dropna(subset=[\"tweet_author_username\"])\n\n\n# Print info about dataset\nprint(f\"Number of total retweets in this dataset: \\n{old_df_len}\")\nprint(f\"\\nTime range of the retweets:\\n{start_date} - {end_date}\")\nprint(f\"\\nKeywords* used to collect the retweets:\\n{search_words}\")\nprint(f\"\\nQuery conditions used to collect the retweets:\\n{query_conds}\")\nprint(f\"\\nNumber of retweets with missing usernames for the original tweeter: {missing_usernames}\\nThese are being dropped from the analysis. New total of retweets: {len(df)}\")\n\n\nNumber of total retweets in this dataset: \n16935\n\nTime range of the retweets:\nFebruary 23, 2023 - May  4, 2023\n\nKeywords* used to collect the retweets:\n(Lehrkräftebildung OR Lehrerbildung OR Lehrkräfte OR Lehrkräftefortbildung OR Seiteneinstieg OR Quereinstieg OR Lehramt)\n\nQuery conditions used to collect the retweets:\n(is:retweet OR is:quote) lang:de\n\nNumber of retweets with missing usernames for the original tweeter: 239\nThese are being dropped from the analysis. New total of retweets: 16696\n\n\nNow let’s prepare the graph.\n\n\nCode\nimport graph_tool.all as gt\n\n# prepare dataset for graph\ndf_ = df.copy()\ndf_[\"weight\"] = df_.groupby(['retweet_author_username', 'tweet_author_username']).transform('size')\n\n# choose columns to keep\ncolumns = ['retweet_author_username',\n    'tweet_author_username',\n    \"weight\",\n    ]\n\ndf_ = df_[columns].drop_duplicates(subset=['retweet_author_username', 'tweet_author_username'])\n\n# create list of edges\ng_list = [(r, t, w, r, t) for r, t, w in zip(df_.retweet_author_username, df_.tweet_author_username, df_.weight)]\n\n# create graph\ngf = gt.Graph(\n    g_list,\n    hashed=True,\n    eprops=[('weight', 'int'), ('retweeter', 'string'), ('tweeter', 'string')]\n)\n\ngf.list_properties()\n\n\nids            (vertex)  (type: string)\nretweeter      (edge)    (type: string)\ntweeter        (edge)    (type: string)\nweight         (edge)    (type: int32_t)"
  },
  {
    "objectID": "src/analysis.html#visualising-the-network",
    "href": "src/analysis.html#visualising-the-network",
    "title": "Twitter Network Analysis",
    "section": "Visualising the network",
    "text": "Visualising the network\n\nThe network as a whole\n\n\nCode\ngt.graph_draw(gf)\n;\n\n\n\n\n\n''\n\n\n\n\nThe largest component of the network\nIf we look at the network as a whole, we can see that there are many isolated nodes on its periphery. If we remove those, this is what we get.\n\n\nCode\n# subgraph of largest component\ng = gt.extract_largest_component(gf, directed=False)\ng = gt.Graph(g, prune=True)\ngt.graph_draw(g)\n;\n\n\n\n\n\n''\n\n\n\n\nDifferent layouts\n\nFruchterman-Reingold layout\n\n\nCode\ngt.graph_draw(g, pos=gt.fruchterman_reingold_layout(g, weight=None, a=None, r=1.0, scale=None, circular=False, grid=True, t_range=None, n_iter=10, pos=None))\n;\n\n\n\n\n\n''\n\n\nComment: super expensive, takes a long time to compute even with 10 iterations (that’s why it looks so bad)\n\n\nSfDP layout\n\n\nCode\ngt.graph_draw(g, pos=gt.sfdp_layout(g, vweight=None, eweight=g.ep[\"weight\"], pin=None, groups=None, C=0.2, K=None, p=2.0, theta=0.6, max_level=15, r=1.0, gamma=0.3, mu=2.0, kappa=1.0, rmap=None, R=1, init_step=None, cooling_step=0.95, adaptive_cooling=True, epsilon=0.01, max_iter=0, pos=None, multilevel=None, coarse_method='hybrid', mivs_thres=0.9, ec_thres=0.75, weighted_coarse=False, verbose=False))\n;\n\n\n\n\n\n''\n\n\n\n\nChoose a layout and save positions\n\npos = gt.sfdp_layout(g, eweight=g.edge_properties[\"weight\"])\n\n# create mapping from usernames to vertex ids\nid_usernames = {v: g.vp.ids[v] for v in range(len(list(g.vp.ids)))}"
  },
  {
    "objectID": "src/analysis.html#statistics-of-the-subnetwork",
    "href": "src/analysis.html#statistics-of-the-subnetwork",
    "title": "Twitter Network Analysis",
    "section": "Statistics of the (sub)network",
    "text": "Statistics of the (sub)network\n\nDegree distribution\n\n\nCode\n# in & out degree distribution\n\nin_hist = gt.vertex_hist(g, \"in\")\nout_hist = gt.vertex_hist(g, \"out\")\n\ny = in_hist[0]\nerr = np.sqrt(in_hist[0])\nplt.errorbar(in_hist[1][:-1], in_hist[0], fmt=\"o\", yerr=err,\n        label=\"in\")\n\ny = out_hist[0]\nerr = np.sqrt(out_hist[0])\nplt.errorbar(out_hist[1][:-1], out_hist[0], fmt=\"o\", yerr=err,\n        label=\"out\")\n\nplt.yscale(\"log\")\nplt.xscale(\"log\")\n\nplt.xlabel(\"$k$\")\nplt.ylabel(\"$NP(k_{in})$\")\n\nplt.tight_layout()\nplt.legend()\n;\n\navg_out = g.get_out_degrees(range(g.num_vertices()), eweight=g.ep[\"weight\"]).mean()\navg_in = g.get_in_degrees(range(g.num_vertices()), eweight=g.ep[\"weight\"]).mean()\n\nprint(f\"Average in-degree: {avg_in}\")\nprint(f\"Average out-degree: {avg_out}\")\n\n\nAverage in-degree: 1.8750587682181477\nAverage out-degree: 1.8750587682181477\n\n\n\n\n\nComment: why the same?? I’m doing something wrong here.\n\n\nWeight distribution\nWeights indicate how many links there are between two nodes. We can see that most nodes have a weight of 1, which means that they have only one link between them.\n\n\nCode\nhist, bins = gt.edge_hist(g, g.ep[\"weight\"], bins=[0, 1], float_count=False)\nbins = bins[:-1]\n\nprint(\"Number of edges with weight:\")\n[print(f\"{b}: {hist[b]}\") for b in range(1, len(bins))]\n\nplot_bar(x=bins, y=hist,\n    title=None,\n    xtick=None, rotation=90,\n    xlabel=None, ylabel=None)\n;\n\n\nNumber of edges with weight:\n1: 12064\n2: 1125\n3: 202\n4: 63\n5: 45\n6: 16\n7: 12\n8: 8\n9: 1\n10: 2\n11: 1\n12: 2\n13: 0\n14: 3\n15: 4\n16: 0\n17: 2\n18: 0\n19: 0\n20: 0\n21: 1\n22: 0\n23: 0\n24: 0\n25: 0\n26: 1\n27: 0\n28: 0\n29: 1\n30: 0\n31: 0\n32: 0\n33: 0\n34: 0\n35: 0\n36: 1\n\n\n''\n\n\n\n\n\n\n\nAverage path length\n\n\nCode\naverage_path_length_in = gt.vertex_average(g, \"in\")\naverage_path_length_out = gt.vertex_average(g, \"out\")\naverage_path_length_total = (gt.vertex_average(g, \"total\"))\n\nprint(f\"Average path length of the network (total): {average_path_length_total[0]} with std: {average_path_length_total[1]}\")\nprint(f\"Average path length of the network (in): {average_path_length_in[0]} with std: {average_path_length_in[1]}\")\nprint(f\"Average path length of the network (out): {average_path_length_out[0]} with std: {average_path_length_out[1]}\")\n\n# plot\n# plt.bar([\"in\", \"out\", \"total\"], [average_path_length_in[0], average_path_length_out[0], average_path_length_total[0]])\n# plt.ylabel(\"Average path length\")\n# plt.title(\"Average path length of the network\")\n\n\nAverage path length of the network (total): 3.1861777150916786 with std: 0.14359437026631217\nAverage path length of the network (in): 1.5930888575458393 with std: 0.131215167205829\nAverage path length of the network (out): 1.5930888575458393 with std: 0.05376431545828757\n\n\nComment: why are in and out the same? The network is directed, so the in and out path lengths should be different, no?\n\n\nShortest distance distribution\nPlot the shortest-distance histogram for each vertex pair in the graph.\n\n\nCode\nhist, bins = gt.distance_histogram(g, weight=None, bins=[0, 1], samples=None, float_count=False)\nbins = bins[:-1]\n\n[print(f\"{b}: {hist[b]}\") for b in range(1, len(bins))]\n\nplot_bar(x=bins, y=hist, width=1,\n    title=\"Shortest distance distribution\",\n    xtick=bins, rotation=90,\n    xlabel=\"Distance\", ylabel=\"Count\")\n;\n\n\n1: 13575\n2: 34321\n3: 58272\n4: 96048\n5: 136822\n6: 171725\n7: 366862\n8: 301350\n9: 312089\n10: 223675\n11: 142258\n12: 74555\n13: 40843\n14: 17901\n15: 9110\n16: 4200\n17: 1614\n18: 486\n19: 117\n20: 18\n21: 1\n\n\n''"
  },
  {
    "objectID": "src/analysis.html#measures-of-centrality",
    "href": "src/analysis.html#measures-of-centrality",
    "title": "Twitter Network Analysis",
    "section": "Measures of centrality",
    "text": "Measures of centrality\n\nDegree centrality\nDegree centrality represents the number of connections a node has.\n\nIn-degree centrality\nIn-degree centrality represents the number of connections going into a node. In the case of retweets, in-degree centrality will indicate that a user is getting a large number of retweets.\n\n\nCode\nin_deg = g.get_in_degrees(g.get_vertices())\n\ndic = {b: in_deg[b] for b in range(1, len(in_deg))}\nvalues_sum = sum(dic.values())\ndic = {key: val for key, val in (sorted(dic.items(), key=lambda x: x[1], reverse=True)[:20])}\n\n# map names to ids\ndic = {id_usernames[k]: v for k, v in dic.items()}\n\n# prepare table\nscore_name = \"In-Degrees\"\ndf_ = pd.DataFrame(dic.items(), columns=[\"Username\", score_name])\ndf_[\"(normalised)\"] = (df_[score_name] / df_[score_name].max()).round(2)\ndf_[\"(share)\"] = ((df_[score_name] / values_sum)*100).round(2)\ndf_.index = np.arange(1, len(df_) + 1)\n\nprint(df_)\n\nplot_bar(x=list(dic.keys()), y=dic.values(), horizontal=True,\n    title=f\"Top 20 users by {score_name}\",\n    xtick=None, rotation=90,\n    xlabel=\"Username\", ylabel=\"Count\");\n;\n\n\n           Username  In-Degrees  (normalised)  (share)\n1     leseerlaubnis         499          1.00     3.68\n2      BentFreiwald         352          0.71     2.60\n3         blume_bob         338          0.68     2.49\n4   Georg_Pazderski         319          0.64     2.35\n5       juliansfrth         251          0.50     1.85\n6    lueckenbildung         235          0.47     1.73\n7     BildungSicher         219          0.44     1.62\n8         DrKissler         201          0.40     1.48\n9        Deu_Kurier         198          0.40     1.46\n10    RicardoLange4         178          0.36     1.31\n11     DurdenGaming         172          0.34     1.27\n12    AnthroBlogger         164          0.33     1.21\n13       WDRaktuell         149          0.30     1.10\n14    AmadeuAntonio         146          0.29     1.08\n15   fuerdieteilung         120          0.24     0.89\n16            dm_ms         111          0.22     0.82\n17  JSchmitzLeipzig         108          0.22     0.80\n18  SozialarbeitNRW         107          0.21     0.79\n19     cote_langues          99          0.20     0.73\n20        FloriKohl          93          0.19     0.69\n\n\n''\n\n\n\n\n\n\n\nOut-degree centrality\nOut-degree centrality represents the number of connections going out of a node. In the case of retweets, out-degree centrality will indicate that a user is retweeting a lot.\n\n\nCode\nout_deg = g.get_out_degrees(g.get_vertices())\n\ndic = {b: out_deg[b] for b in range(1, len(out_deg))}\nvalues_sum = sum(dic.values())\ndic = {key: val for key, val in (sorted(dic.items(), key=lambda x: x[1], reverse=True)[:20])}\n\n# map names to ids\ndic = {id_usernames[k]: v for k, v in dic.items()}\n\n# prepare table\nscore_name = \"Out-Degrees\"\ndf_ = pd.DataFrame(dic.items(), columns=[\"Username\", score_name])\ndf_[\"(normalised)\"] = (df_[score_name] / df_[score_name].max()).round(2)\ndf_[\"(share)\"] = ((df_[score_name] / values_sum)*100).round(2)\ndf_.index = np.arange(1, len(df_) + 1)\n\nplot_bar(x=list(dic.keys()), y=dic.values(), horizontal=True,\n    title=f\"Top 20 users by {score_name}\",\n    xtick=None, rotation=90,\n    xlabel=\"Username\", ylabel=\"Count\");\n;\n\nprint(df_)\n\n\n           Username  Out-Degrees  (normalised)  (share)\n1     Bot_TwLehrerZ          418          1.00     3.08\n2          Doriiien           42          0.10     0.31\n3          goesicke           40          0.10     0.30\n4            Ju1820           34          0.08     0.25\n5          lothenon           30          0.07     0.22\n6      Mandala81966           29          0.07     0.21\n7   Arsenio57509279           25          0.06     0.18\n8           lgoshen           24          0.06     0.18\n9           seni_bl           24          0.06     0.18\n10      nicole37911           24          0.06     0.18\n11     miaumiau1975           24          0.06     0.18\n12    KinderJournal           23          0.06     0.17\n13         eduBWbot           21          0.05     0.15\n14        FloriKohl           21          0.05     0.15\n15         sanjosko           21          0.05     0.15\n16           _Wir_4           21          0.05     0.15\n17    SvenDudkowiak           20          0.05     0.15\n18   julian07646263           20          0.05     0.15\n19         printi71           20          0.05     0.15\n20     ThomasKnorra           20          0.05     0.15\n\n\n\n\n\nComment: seems like a bot is retweeting a lot more than any other user.\n\n\n\nBetweenness centrality\nBetweeness centrality represents the number of all ‘shortest paths’ between nodes that pass through a specific node. In other words, it counts how often that node is part of a short connection. In the case of retweets, it measures the extent to which a user connects different communities of users.\n\nUseful when there is flux / information flow within network\nHelps predict and locate vulnerability of a network (i.e. if you remove a node with high betweeness)\nProblem: computationally expensive (as you need to calculate all shortest paths)\n\n\n\nCode\nvp, ep = gt.betweenness(g, pivots=None, vprop=None, eprop=None, weight=g.ep[\"weight\"], norm=True)\n\ngt.graph_draw(g, pos=pos, vertex_fill_color=vp,\n              vertex_size=gt.prop_to_size(vp, mi=1, ma=10),\n              edge_pen_width=gt.prop_to_size(ep, mi=0.5, ma=5),\n              vcmap=plt.cm.autumn,\n              vorder=vp)\n;\n\n\n\n\n\n''\n\n\nCalculate top users by betweenness centrality\n\n\nCode\ndic = {i: vp.a[i] for i in range(g.num_vertices())}\ndic = {key: val for key, val in (sorted(dic.items(), key=lambda x: x[1], reverse=True)[:20])}\n\n# map names to ids\ndic = {id_usernames[k]: v for k, v in dic.items()}\n\n# prepare table\nscore_name = \"Betweenness Score\"\ndf_ = pd.DataFrame(dic.items(), columns=[\"Username\", score_name])\ndf_[\"(normalised)\"] = (df_[score_name] / df_[score_name].max()).round(2)\ndf_ = df_.round(5)\ndf_.index = np.arange(1, len(df_) + 1)\n\nprint(df_)\n\nplot_bar(x=list(dic.keys()), y=dic.values(), horizontal=True,\n    title=f\"Top 20 users by {score_name}\",\n    xtick=None, rotation=90,\n    xlabel=\"Username\", ylabel=\"Count\")\n;\n\n\n           Username  Betweenness Score  (normalised)\n1             ciffi            0.01809          1.00\n2          Woe_Real            0.01642          0.91\n3     Bot_TwLehrerZ            0.01625          0.90\n4     DerMedienwart            0.01614          0.89\n5     leseerlaubnis            0.00840          0.46\n6            _Wir_4            0.00710          0.39\n7        viethentus            0.00541          0.30\n8   telekomstiftung            0.00439          0.24\n9       MarkRackles            0.00385          0.21\n10   susanneposselt            0.00375          0.21\n11  lernendigitalDE            0.00353          0.20\n12        blume_bob            0.00322          0.18\n13           DBS_20            0.00305          0.17\n14      hav_hendrik            0.00287          0.16\n15        FloriKohl            0.00283          0.16\n16  JSchmitzLeipzig            0.00259          0.14\n17      Herr_Nuxoll            0.00216          0.12\n18   regina_kittler            0.00187          0.10\n19   HSE_Heidelberg            0.00186          0.10\n20        BMBF_Bund            0.00185          0.10\n\n\n''\n\n\n\n\n\nCalculate top edges by betweenness centrality\n\n\nCode\ndic = {i: ep.a[i] for i in range(g.num_edges())}\ndic = {key: val for key, val in (sorted(dic.items(), key=lambda x: x[1], reverse=True)[:20])}\n\n# get names of tweeters and retweeters\nlis = [(list(g.ep.tweeter)[k], list(g.ep.retweeter)[k], v) for k, v in dic.items()]\n\n# prepare table\nscore_name = \"Betweenness Score\"\ndf_ = pd.DataFrame(lis, columns=[\"Tweeter\", \"Retweeter\", score_name])\ndf_[\"(normalised)\"] = (df_[score_name] / df_[score_name].max()).round(2)\ndf_ = df_.round(5)\ndf_.index = np.arange(1, len(df_) + 1)\n\nprint(df_)\n\nplot_bar(x=df_.index, y=df_[score_name],\n    title=f\"Top 20 edges by {score_name}\",\n    xtick=df_.index, rotation=90,\n    xlabel=\"Edges\", ylabel=\"Count\");\n\n\n            Tweeter        Retweeter  Betweenness Score  (normalised)\n1          Woe_Real            ciffi            0.01643          1.00\n2     Bot_TwLehrerZ    DerMedienwart            0.01610          0.98\n3     DerMedienwart         Woe_Real            0.01559          0.95\n4            _Wir_4    leseerlaubnis            0.00713          0.43\n5             ciffi       viethentus            0.00539          0.33\n6             ciffi           _Wir_4            0.00489          0.30\n7        viethentus  telekomstiftung            0.00428          0.26\n8   telekomstiftung      MarkRackles            0.00386          0.23\n9       MarkRackles        blume_bob            0.00315          0.19\n10           DBS_20  lernendigitalDE            0.00308          0.19\n11            ciffi   susanneposselt            0.00249          0.15\n12      hav_hendrik  JSchmitzLeipzig            0.00248          0.15\n13   susanneposselt           DBS_20            0.00246          0.15\n14            ciffi      Herr_Nuxoll            0.00211          0.13\n15      Herr_Nuxoll      hav_hendrik            0.00203          0.12\n16  lernendigitalDE        BMBF_Bund            0.00185          0.11\n17    leseerlaubnis         JacyLu76            0.00152          0.09\n18  JSchmitzLeipzig     paediatrice1            0.00151          0.09\n19     paediatrice1    BildungSicher            0.00151          0.09\n20         JacyLu76       trullateee            0.00148          0.09\n\n\n\n\n\n\n\nEigenvalue centrality\nSum of all the eigenvector centralities of its neighbours. My importance is dependent on the importance of the nodes which I am connected to. It is recursive: so my centrality depends on the centrality of my neighbours, which depends on the centrality of their neighbours, and so on. Takes into account the whole network.\n\n\nCode\ng.vertex_properties[\"eigenvector\"] = g.new_vertex_property(\"double\")\n\nmax_eigenvalue, _ = gt.eigenvector(g, weight=g.ep[\"weight\"], vprop=g.vp[\"eigenvector\"], epsilon=1e-6, max_iter=1000)\n\nprint(max_eigenvalue)\n\n\n0.0\n\n\nComment: eigenvalues are returning all 0s. Why?\n\n\nKatz centrality\nKatz: makes use of the adjacency matrix. Similar to eigenvector, but includes an alpha factor that gradually reduces the influence of further nodes. So the difference is that you can set (through alpha) how much influence local neighbours have in comparison to far away neighbours.\n\n\nCode\nimport seaborn as sns\n\nalphas = [0.1, 0.01, 0.001]\n\ndf_merged = pd.DataFrame()\n\nfor i, alpha in enumerate(alphas):\n\n    vp_name = f\"katz_{alpha}\"\n    g.vertex_properties[vp_name] = g.new_vertex_property(\"double\")\n\n    gt.katz(g, alpha=alpha, beta=None, weight=g.ep[\"weight\"], vprop=g.vp[vp_name], epsilon=1e-06, max_iter=None, norm=True)\n\n    dic = {i: g.vp[vp_name][i] for i in range(len(list(g.vp[vp_name])))}\n    #dic = {key: val for key, val in (sorted(dic.items(), key=lambda x: x[1], reverse=True))}\n    dic = {id_usernames[k]: v for k, v in dic.items()}\n\n    # prepare table\n    score_name = f\"Katz Score (a={alpha})\"\n    df_ = pd.DataFrame(dic.items(), columns=[\"Username\", score_name])\n    #df_[\"(normalised)\"] = (df_[score_name] / df_[score_name].max()).round(2)\n    df_ = df_.round(5)\n    df_.index = np.arange(1, len(df_) + 1)\n\n    if i == 0:\n        df_merged = df_\n    else:\n        # df_ = df_[[\"Username\", score_name]]\n        df_merged = df_merged.merge(df_, on=\"Username\", how=\"left\", suffixes=(\"\", f\"_{alpha}\"))\n\ndf_top20 = df_merged.sort_values(by=df_merged.columns[1], ascending=False).head(20)\n\nprint(df_top20)\n\n\np = sns.catplot(\n    data=df_top20.melt(id_vars='Username'), kind=\"bar\",\n    y=\"Username\", x=\"value\", hue=\"variable\",\n    legend_out=False, aspect=1.5\n)\nplt.title(\"Katz centrality for different alpha values\")\n;\n\n\n/home/ubuntu-user/miniconda3/envs/tna/lib/python3.10/site-packages/graph_tool/centrality/__init__.py:757: RuntimeWarning: invalid value encountered in divide\n  vprop.fa = vprop.fa / numpy.linalg.norm(vprop.fa)\n\n\n           Username  Katz Score (a=0.1)  Katz Score (a=0.01)   \n0            OSynje                 0.0              0.01051  \\\n5671     Shirzan_22                 0.0              0.01051   \n5685      Netti_SGE                 0.0              0.01051   \n5684     jansen_tja                 0.0              0.01051   \n5683  marco_tralles                 0.0              0.01051   \n5682  ShouraHashemi                 0.0              0.01063   \n5681      euroberan                 0.0              0.01051   \n5680    EssjSongoku                 0.0              0.01051   \n5679   SunnyDayYeah                 0.0              0.01051   \n5678    Shelly_Pond                 0.0              0.01136   \n5677   nassimhelene                 0.0              0.01051   \n5676  DanielZTwitta                 0.0              0.01051   \n5675    wifeofanagp                 0.0              0.01051   \n5674     Nendia1989                 0.0              0.01051   \n5673      Hoge_Kijo                 0.0              0.01051   \n5672    Umblaettern                 0.0              0.01062   \n5670    WerderLiebe                 0.0              0.01063   \n5720     VerbundFDB                 0.0              0.01094   \n5669  mantra_slider                 0.0              0.01051   \n5668    BirgitNsaka                 0.0              0.01051   \n\n      Katz Score (a=0.001)  \n0                  0.01082  \n5671               0.01082  \n5685               0.01082  \n5684               0.01082  \n5683               0.01082  \n5682               0.01083  \n5681               0.01082  \n5680               0.01082  \n5679               0.01082  \n5678               0.01091  \n5677               0.01082  \n5676               0.01082  \n5675               0.01082  \n5674               0.01082  \n5673               0.01082  \n5672               0.01083  \n5670               0.01083  \n5720               0.01086  \n5669               0.01082  \n5668               0.01082  \n\n\n''\n\n\n\n\n\n\n\nPagerank\nSimilar to eigenvector. For a random walker on a network, what is the expected probability of it being on that node, plus the probability that it jumps to somewhere completely different. The smaller the alpha, the more it jumps around. The higher the number that walker passes through a node, the higher the centrality of that node.\nAdding some noise makse sure results does not point you to the same nodes every time.\n\n\nCode\ng.vertex_properties[\"pagerank\"] = g.new_vertex_property(\"double\")\n\ngt.pagerank(g, damping=0.85, pers=None, weight=g.ep[\"weight\"], prop=g.vp[\"pagerank\"], epsilon=1e-06, max_iter=None, ret_iter=False)\n\ndic = {i: g.vp[\"pagerank\"][i] for i in range(len(list(g.vp[\"pagerank\"])))}\ndic = {key: val for key, val in (sorted(dic.items(), key=lambda x: x[1], reverse=True)[:20])}\n\n# map names to ids\ndic = {id_usernames[k]: v for k, v in dic.items()}\n\n# prepare table\nscore_name = \"Pagerank Score\"\ndf_ = pd.DataFrame(dic.items(), columns=[\"Username\", score_name])\ndf_[\"(normalised)\"] = (df_[score_name] / df_[score_name].max()).round(2)\ndf_ = df_.round(5)\ndf_.index = np.arange(1, len(df_) + 1)\n\nprint(df_)\n\nplot_bar(x=dic.keys(), y=dic.values(),\n    title=f\"Top 20 users by {score_name}\",\n    xtick=None, rotation=90,\n    xlabel=\"Username\", ylabel=\"Count\")\n;\n\n\n           Username  Pagerank Score  (normalised)\n1       juliansfrth         0.05447          1.00\n2    ARD_Presseclub         0.03091          0.57\n3      BentFreiwald         0.03059          0.56\n4     AnthroBlogger         0.02944          0.54\n5    prof_m_baumann         0.02642          0.49\n6      martingommel         0.02271          0.42\n7     BildungSicher         0.01550          0.28\n8     leseerlaubnis         0.01298          0.24\n9       rosenbusch_         0.01280          0.23\n10        blume_bob         0.01110          0.20\n11  Georg_Pazderski         0.00896          0.16\n12   lueckenbildung         0.00816          0.15\n13      watch_union         0.00750          0.14\n14   ernsterjuenger         0.00644          0.12\n15        DrKissler         0.00609          0.11\n16       Deu_Kurier         0.00551          0.10\n17    RicardoLange4         0.00473          0.09\n18     DurdenGaming         0.00452          0.08\n19    Anna_Lena2022         0.00419          0.08\n20  AladinMafaalani         0.00402          0.07\n\n\n''"
  },
  {
    "objectID": "src/analysis.html#other-stuff-i.e.-i-dont-know-what-im-doing",
    "href": "src/analysis.html#other-stuff-i.e.-i-dont-know-what-im-doing",
    "title": "Twitter Network Analysis",
    "section": "Other stuff (i.e. I don’t know what I’m doing)",
    "text": "Other stuff (i.e. I don’t know what I’m doing)\n\nLocal clustering\n\n\nCode\ng.vertex_properties[\"local_clustering\"] = g.new_vertex_property(\"double\")\n\ngt.local_clustering(g, prop=g.vp[\"local_clustering\"], undirected=False)\n\n\n&lt;VertexPropertyMap object with value type 'double', for Graph 0x7f42357babf0, at 0x7f422bf7d3f0&gt;\n\n\nComments: how do I use this?\n\n\nPseudo-diameter\n\n\nCode\ngt.pseudo_diameter(g,  source=None, weights=g.ep[\"weight\"])\n\n\n(1.0,\n (&lt;Vertex object with index '0' at 0x7f422bf502c0&gt;,\n  &lt;Vertex object with index '466' at 0x7f422bf50240&gt;))\n\n\n\n\nRadial layout\n\n\nCode\nroot = g.vertex(101) # username: leseerlaubnis\n\ngt.graph_draw(g, pos=gt.radial_tree_layout(g, root, rel_order=None, rel_order_leaf=False, weighted=False, node_weight=None, r=1.0))\n\n\n\n\n\n&lt;VertexPropertyMap object with value type 'vector&lt;double&gt;', for Graph 0x7f42357babf0, at 0x7f42301e1930&gt;\n\n\nComment: what does that show, exactly?\n\n\nBlockmodel\n\n\nCode\nstate = gt.minimize_blockmodel_dl(g)\n\nstate.draw(vertex_shape=state.get_blocks())\n\n\n\n\n\n&lt;VertexPropertyMap object with value type 'vector&lt;double&gt;', for Graph 0x7f42357babf0, at 0x7f422bf577f0&gt;\n\n\n\n\nCode\nstate2 = gt.minimize_nested_blockmodel_dl(g)\n\nstate2.draw()\n\n\n\n\n\n(&lt;VertexPropertyMap object with value type 'vector&lt;double&gt;', for Graph 0x7f42357babf0, at 0x7f42e02c8400&gt;,\n &lt;GraphView object, directed, with 8534 vertices and 8533 edges, edges filtered by (&lt;EdgePropertyMap object with value type 'bool', for Graph 0x7f42e02c8b80, at 0x7f42e02c9d20&gt;, False), vertices filtered by (&lt;VertexPropertyMap object with value type 'bool', for Graph 0x7f42e02c8b80, at 0x7f42e02c9210&gt;, False), at 0x7f42e02c8b80&gt;,\n &lt;VertexPropertyMap object with value type 'vector&lt;double&gt;', for Graph 0x7f42e02c8b80, at 0x7f42e02cb190&gt;)\n\n\nComment: What is going on here??\n\n\nCode\nstate = gt.minimize_nested_blockmodel_dl(g)\n\nt = gt.get_hierarchy_tree(state)[0]\n\ntpos = pos = gt.radial_tree_layout(t, t.vertex(t.num_vertices() - 1, use_index=False), weighted=True)\n\ncts = gt.get_hierarchy_control_points(g, t, tpos)\n\npos = g.own_property(tpos)\n\nb = state.levels[0].b\n\nshape = b.copy()\n\nshape.a %= 14\n\ngt.graph_draw(g, pos=pos, vertex_fill_color=b, vertex_shape=shape, edge_control_points=cts,\nedge_color=[0, 0, 0, 0.3], vertex_anchor=0)\n\n\n\n\n\n&lt;VertexPropertyMap object with value type 'vector&lt;double&gt;', for Graph 0x7f42357babf0, at 0x7f42301871c0&gt;\n\n\n\n\nEdge percolation\n\n\nCode\nedges = sorted([(e.source(), e.target()) for e in g.edges()],\n               key=lambda e: e[0].in_degree() * e[1].in_degree())\n\nsizes, comp = gt.edge_percolation(g, edges)\n\nnp.random.shuffle(edges)\n\nsizes2, comp = gt.edge_percolation(g, edges)\n\nplt.plot(sizes, label=\"Targeted\")\n\nplt.plot(sizes2, label=\"Random\")\n\nplt.legend()\n\n\n&lt;matplotlib.legend.Legend at 0x7f4230644850&gt;\n\n\n\n\n\n\n\nK-core decpomposition\n\n\nCode\nkcore = gt.kcore_decomposition(g)\ngt.graph_draw(g, vertex_fill_color=kcore, vertex_text=kcore)\n\n\n\n\n\n&lt;VertexPropertyMap object with value type 'vector&lt;double&gt;', for Graph 0x7f42357babf0, at 0x7f422bf00880&gt;"
  },
  {
    "objectID": "src/analysis.html#comments-issues",
    "href": "src/analysis.html#comments-issues",
    "title": "Twitter Network Analysis",
    "section": "COMMENTS / ISSUES",
    "text": "COMMENTS / ISSUES"
  }
]