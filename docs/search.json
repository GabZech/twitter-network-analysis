[
  {
    "objectID": "src/analysis.html",
    "href": "src/analysis.html",
    "title": "Twitter Network Analysis",
    "section": "",
    "text": "First, let’s define some helper functions to be used later.\n\n\nCode\nimport json\nimport re\n\nimport numpy as np\nimport pandas as pd\n\ndef extract_missing_usernames(df, username_column):\n    pattern = r\"RT @([A-Za-z0-9_]+):\"\n    usernames = []\n    for index, row in df.iterrows():\n        match = re.search(pattern, row[\"retweet_text\"])\n        if match:\n            usernames.append(match.group(1))\n        else:\n            usernames.append(row[username_column])\n    df[username_column] = usernames\n\n    return df\n\n\ndef get_time_range(df):\n    \"\"\"Get the time range of the DataFrame\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame to be queried\n    date_lang : str, optional\n        Language of the date, by default \"de_DE\"\n\n    Returns\n    -------\n    str\n        Start date of the DataFrame\n    str\n        End date of the DataFrame\n    \"\"\"\n    df.retweet_created_at = df.retweet_created_at.astype(\"datetime64[ns, UTC]\")\n    start_date = df.retweet_created_at.min().strftime(\"%B %e, %Y\")\n    end_date = df.retweet_created_at.max().strftime(\"%B %e, %Y\")\n\n    return start_date, end_date\n\n\ndef get_largest_values(df, col_name, n):\n    \"\"\"Get the n largest values of a column in a DataFrame\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame to be queried\n    col_name : str\n        Name of the column to be queried\n    n : int\n        Number of largest values to be returned (i.e. number of rows)\n\n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame with the n largest values of the column\n    \"\"\"\n    top = df.sort_values(col_name, ascending=False).head(n)\n\n    # put col_name as first column\n    cols = top.columns.tolist()\n    cols = cols[-1:] + cols[:-1]\n    top = top[cols]\n\n    return top\n\n\ndef get_top_users(df, df_authors, column_name, n):\n    \"\"\"Get the top n users with their profiles based on a column in a DataFrame\n    Values of the column are standardized so that the largest value is 1.0\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame to be queried\n    df_authors : pandas.DataFrame\n        DataFrame with the usernames and names of authors\n    column_name : str\n        Name of the column to be queried\n    n : int\n        Number of largest values to be returned (i.e. number of rows)\n\n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame with the n largest values of the column\n    \"\"\"\n    column_name_std = column_name + \" (normalised)\"\n    df[column_name_std] = df[column_name] / max(df[column_name])\n    df = get_largest_values(df, column_name, n)\n    df = add_profile_url(df, \"username\")\n    df = pd.merge(df, df_authors, on=\"username\", how=\"left\")\n    df = df.round(5)\n    df.index = np.arange(1, len(df) + 1)\n    df = df[[column_name, column_name_std, \"username\", \"name\", \"profile_url\"]]\n    return df\n\n\ndef get_authors_name(df):\n    \"\"\"Get the usernames and names of retweet authors and tweet authors\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame to be queried\n\n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame with the usernames and names of retweet authors and tweet authors\n    \"\"\"\n    retweet_authors = df[[\"retweet_author_username\", \"retweet_author_name\"]].copy()\n    retweet_authors.rename(\n        columns={\"retweet_author_username\": \"username\", \"retweet_author_name\": \"name\"},\n        inplace=True,\n    )\n\n    tweet_authors = df[[\"tweet_author_username\", \"tweet_author_name\"]].copy()\n    tweet_authors.rename(\n        columns={\"tweet_author_username\": \"username\", \"tweet_author_name\": \"name\"},\n        inplace=True,\n    )\n\n    authors = pd.concat([retweet_authors, tweet_authors])\n    authors = authors.drop_duplicates(subset=[\"username\"], keep=\"last\").reset_index(\n        drop=True\n    )\n\n    return authors\n\n\ndef add_profile_url(df, username_col):\n    df[\"profile_url\"] = \"https://twitter.com/\" + df[username_col]\n\n    return df\n\n\nNow let’s prepare the dataset and print some relevant information about it.\n\n\nCode\n# load and clean dataset\ndf = pd.read_parquet(\"data/raw/all_tweets_lehrkraeftebildung.parquet\")\ndf.replace([\"NaN\", \"nan\", \"None\", \"\"], np.NaN, inplace=True)\ndf = extract_missing_usernames(df, \"tweet_author_username\")\n\n# get information about the retweets\nstart_date, end_date = get_time_range(df)\nsearch_words = \"(Lehrkräftebildung OR Lehrerbildung OR Lehrkräfte OR Lehrkräftefortbildung OR Seiteneinstieg OR Quereinstieg OR Lehramt)\"\nquery_conds = \"(is:retweet OR is:quote) lang:de\"\n\n# drop retweets with missing usernames\nold_df_len = df.shape[0]\ntry:\n    missing_usernames = df.tweet_author_username.isnull().value_counts()[True]\nexcept KeyError:\n    missing_usernames = 0\ndf = df.dropna(subset=[\"tweet_author_username\"])\n\n\n# Print info about dataset\nprint(f\"Number of total retweets in this dataset: \\n{old_df_len}\")\nprint(f\"\\nTime range of the retweets:\\n{start_date} - {end_date}\")\nprint(f\"\\nKeywords* used to collect the retweets:\\n{search_words}\")\nprint(f\"\\nQuery conditions used to collect the retweets:\\n{query_conds}\")\nprint(f\"\\nNumber of retweets with missing usernames for the original tweeter: {missing_usernames}\\nThese are being dropped from the analysis. New total of retweets: {len(df)}\")\n\n\nNumber of total retweets in this dataset: \n11027\n\nTime range of the retweets:\nFebruary 23, 2023 - April  1, 2023\n\nKeywords* used to collect the retweets:\n(Lehrkräftebildung OR Lehrerbildung OR Lehrkräfte OR Lehrkräftefortbildung OR Seiteneinstieg OR Quereinstieg OR Lehramt)\n\nQuery conditions used to collect the retweets:\n(is:retweet OR is:quote) lang:de\n\nNumber of retweets with missing usernames for the original tweeter: 199\nThese are being dropped from the analysis. New total of retweets: 10828\n\n\nNow let’s prepare the graph.\n\n\nCode\nimport graph_tool.all as gt\n\n# prepare dataset for graph\ndf_ = df.copy()\ndf_[\"weight\"] = df_.groupby(['retweet_author_username', 'tweet_author_username']).transform('size')\n\n# choose columns to keep\ncolumns = ['retweet_author_username',\n    'tweet_author_username',\n    \"weight\",\n    ]\n\ndf_ = df_[columns].drop_duplicates(subset=['retweet_author_username', 'tweet_author_username'])\n\n# create list of edges\ng_list = [(r, t, w, r, t) for r, t, w in zip(df_.retweet_author_username, df_.tweet_author_username, df_.weight)]\n\n# create graph\ng = gt.Graph(\n    g_list,\n    hashed=True,\n    eprops=[('weight', 'int'), ('retweeter', 'string'), ('tweeter', 'string')]\n)"
  },
  {
    "objectID": "src/analysis.html#data-preparation",
    "href": "src/analysis.html#data-preparation",
    "title": "Twitter Network Analysis",
    "section": "",
    "text": "First, let’s define some helper functions to be used later.\n\n\nCode\nimport json\nimport re\n\nimport numpy as np\nimport pandas as pd\n\ndef extract_missing_usernames(df, username_column):\n    pattern = r\"RT @([A-Za-z0-9_]+):\"\n    usernames = []\n    for index, row in df.iterrows():\n        match = re.search(pattern, row[\"retweet_text\"])\n        if match:\n            usernames.append(match.group(1))\n        else:\n            usernames.append(row[username_column])\n    df[username_column] = usernames\n\n    return df\n\n\ndef get_time_range(df):\n    \"\"\"Get the time range of the DataFrame\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame to be queried\n    date_lang : str, optional\n        Language of the date, by default \"de_DE\"\n\n    Returns\n    -------\n    str\n        Start date of the DataFrame\n    str\n        End date of the DataFrame\n    \"\"\"\n    df.retweet_created_at = df.retweet_created_at.astype(\"datetime64[ns, UTC]\")\n    start_date = df.retweet_created_at.min().strftime(\"%B %e, %Y\")\n    end_date = df.retweet_created_at.max().strftime(\"%B %e, %Y\")\n\n    return start_date, end_date\n\n\ndef get_largest_values(df, col_name, n):\n    \"\"\"Get the n largest values of a column in a DataFrame\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame to be queried\n    col_name : str\n        Name of the column to be queried\n    n : int\n        Number of largest values to be returned (i.e. number of rows)\n\n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame with the n largest values of the column\n    \"\"\"\n    top = df.sort_values(col_name, ascending=False).head(n)\n\n    # put col_name as first column\n    cols = top.columns.tolist()\n    cols = cols[-1:] + cols[:-1]\n    top = top[cols]\n\n    return top\n\n\ndef get_top_users(df, df_authors, column_name, n):\n    \"\"\"Get the top n users with their profiles based on a column in a DataFrame\n    Values of the column are standardized so that the largest value is 1.0\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame to be queried\n    df_authors : pandas.DataFrame\n        DataFrame with the usernames and names of authors\n    column_name : str\n        Name of the column to be queried\n    n : int\n        Number of largest values to be returned (i.e. number of rows)\n\n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame with the n largest values of the column\n    \"\"\"\n    column_name_std = column_name + \" (normalised)\"\n    df[column_name_std] = df[column_name] / max(df[column_name])\n    df = get_largest_values(df, column_name, n)\n    df = add_profile_url(df, \"username\")\n    df = pd.merge(df, df_authors, on=\"username\", how=\"left\")\n    df = df.round(5)\n    df.index = np.arange(1, len(df) + 1)\n    df = df[[column_name, column_name_std, \"username\", \"name\", \"profile_url\"]]\n    return df\n\n\ndef get_authors_name(df):\n    \"\"\"Get the usernames and names of retweet authors and tweet authors\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame to be queried\n\n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame with the usernames and names of retweet authors and tweet authors\n    \"\"\"\n    retweet_authors = df[[\"retweet_author_username\", \"retweet_author_name\"]].copy()\n    retweet_authors.rename(\n        columns={\"retweet_author_username\": \"username\", \"retweet_author_name\": \"name\"},\n        inplace=True,\n    )\n\n    tweet_authors = df[[\"tweet_author_username\", \"tweet_author_name\"]].copy()\n    tweet_authors.rename(\n        columns={\"tweet_author_username\": \"username\", \"tweet_author_name\": \"name\"},\n        inplace=True,\n    )\n\n    authors = pd.concat([retweet_authors, tweet_authors])\n    authors = authors.drop_duplicates(subset=[\"username\"], keep=\"last\").reset_index(\n        drop=True\n    )\n\n    return authors\n\n\ndef add_profile_url(df, username_col):\n    df[\"profile_url\"] = \"https://twitter.com/\" + df[username_col]\n\n    return df\n\n\nNow let’s prepare the dataset and print some relevant information about it.\n\n\nCode\n# load and clean dataset\ndf = pd.read_parquet(\"data/raw/all_tweets_lehrkraeftebildung.parquet\")\ndf.replace([\"NaN\", \"nan\", \"None\", \"\"], np.NaN, inplace=True)\ndf = extract_missing_usernames(df, \"tweet_author_username\")\n\n# get information about the retweets\nstart_date, end_date = get_time_range(df)\nsearch_words = \"(Lehrkräftebildung OR Lehrerbildung OR Lehrkräfte OR Lehrkräftefortbildung OR Seiteneinstieg OR Quereinstieg OR Lehramt)\"\nquery_conds = \"(is:retweet OR is:quote) lang:de\"\n\n# drop retweets with missing usernames\nold_df_len = df.shape[0]\ntry:\n    missing_usernames = df.tweet_author_username.isnull().value_counts()[True]\nexcept KeyError:\n    missing_usernames = 0\ndf = df.dropna(subset=[\"tweet_author_username\"])\n\n\n# Print info about dataset\nprint(f\"Number of total retweets in this dataset: \\n{old_df_len}\")\nprint(f\"\\nTime range of the retweets:\\n{start_date} - {end_date}\")\nprint(f\"\\nKeywords* used to collect the retweets:\\n{search_words}\")\nprint(f\"\\nQuery conditions used to collect the retweets:\\n{query_conds}\")\nprint(f\"\\nNumber of retweets with missing usernames for the original tweeter: {missing_usernames}\\nThese are being dropped from the analysis. New total of retweets: {len(df)}\")\n\n\nNumber of total retweets in this dataset: \n11027\n\nTime range of the retweets:\nFebruary 23, 2023 - April  1, 2023\n\nKeywords* used to collect the retweets:\n(Lehrkräftebildung OR Lehrerbildung OR Lehrkräfte OR Lehrkräftefortbildung OR Seiteneinstieg OR Quereinstieg OR Lehramt)\n\nQuery conditions used to collect the retweets:\n(is:retweet OR is:quote) lang:de\n\nNumber of retweets with missing usernames for the original tweeter: 199\nThese are being dropped from the analysis. New total of retweets: 10828\n\n\nNow let’s prepare the graph.\n\n\nCode\nimport graph_tool.all as gt\n\n# prepare dataset for graph\ndf_ = df.copy()\ndf_[\"weight\"] = df_.groupby(['retweet_author_username', 'tweet_author_username']).transform('size')\n\n# choose columns to keep\ncolumns = ['retweet_author_username',\n    'tweet_author_username',\n    \"weight\",\n    ]\n\ndf_ = df_[columns].drop_duplicates(subset=['retweet_author_username', 'tweet_author_username'])\n\n# create list of edges\ng_list = [(r, t, w, r, t) for r, t, w in zip(df_.retweet_author_username, df_.tweet_author_username, df_.weight)]\n\n# create graph\ng = gt.Graph(\n    g_list,\n    hashed=True,\n    eprops=[('weight', 'int'), ('retweeter', 'string'), ('tweeter', 'string')]\n)"
  },
  {
    "objectID": "src/analysis.html#visualising-the-network",
    "href": "src/analysis.html#visualising-the-network",
    "title": "Twitter Network Analysis",
    "section": "Visualising the network",
    "text": "Visualising the network\n\nThe network as a whole\n\n\nCode\ngt.graph_draw(g)\n\n\n\n\n\n&lt;VertexPropertyMap object with value type 'vector&lt;double&gt;', for Graph 0x7f518c0e74f0, at 0x7f5151fc92d0&gt;\n\n\n\n\nThe largest component of the network\nIf we look at the network as a whole, we can see that there are many isolated nodes on its periphery. If we remove those, this is what we get.\n\n\nCode\n# subgraph of largest component\ng = gt.extract_largest_component(g, directed=False)\ng = gt.Graph(g, prune=True)\npos = gt.sfdp_layout(g, eweight=g.edge_properties[\"weight\"])\n\ngt.graph_draw(g, pos)\n\n# create mapping from usernames to vertex ids\nid_usernames = {v: g.vp.ids[v] for v in range(len(list(g.vp.ids)))}"
  },
  {
    "objectID": "src/analysis.html#statistics-of-the-subnetwork",
    "href": "src/analysis.html#statistics-of-the-subnetwork",
    "title": "Twitter Network Analysis",
    "section": "Statistics of the (sub)network",
    "text": "Statistics of the (sub)network\n\nDegree distribution\n\n\nCode\nimport matplotlib.pyplot as plt\n\n# in & out degree distribution\n\nin_hist = gt.vertex_hist(g, \"in\")\nout_hist = gt.vertex_hist(g, \"out\")\n\ny = in_hist[0]\nerr = np.sqrt(in_hist[0])\nplt.errorbar(in_hist[1][:-1], in_hist[0], fmt=\"o\", yerr=err,\n        label=\"in\")\n\ny = out_hist[0]\nerr = np.sqrt(out_hist[0])\nplt.errorbar(out_hist[1][:-1], out_hist[0], fmt=\"o\", yerr=err,\n        label=\"out\")\n\nplt.yscale(\"log\")\nplt.xscale(\"log\")\n\nplt.xlabel(\"$k$\")\nplt.ylabel(\"$NP(k_{in})$\")\n\nplt.tight_layout()\nplt.legend()\n\n\n&lt;matplotlib.legend.Legend at 0x7f5155999cc0&gt;"
  },
  {
    "objectID": "src/analysis.html#measures-of-centrality",
    "href": "src/analysis.html#measures-of-centrality",
    "title": "Twitter Network Analysis",
    "section": "Measures of centrality",
    "text": "Measures of centrality\n\nBetweenness centrality\nBetweeness centrality represents the number of all ‘shortest paths’ between nodes that pass through a specific node. In other words, it counts how often that node is part of a short connection. In the case of retweets, it measures the extent to which a user connects different communities of users.\n\nUseful when there is flux / information flow within network\nHelps predict and locate vulnerability of a network (i.e. if you remove a node with high betweeness)\nProblem: computationally expensive (as you need to calculate all shortest paths)\n\n\n\nCode\nvp, ep = gt.betweenness(g)\n\ngt.graph_draw(g, pos=pos, vertex_fill_color=vp,\n              vertex_size=gt.prop_to_size(vp, mi=1, ma=10),\n              edge_pen_width=gt.prop_to_size(ep, mi=0.5, ma=5),\n              vcmap=plt.cm.autumn,\n              vorder=vp)\n\n\n\n\n\n&lt;VertexPropertyMap object with value type 'vector&lt;double&gt;', for Graph 0x7f5151fcb370, at 0x7f51c03113c0&gt;\n\n\nCalculate top users betweenness centrality\n\n\nCode\ndic = {i: vp.a[i] for i in range(g.num_vertices())}\ndic = {key: val for key, val in (sorted(dic.items(), key=lambda x: x[1], reverse=True)[:20])}\n\n# map names to ids\ndic = {id_usernames[k]: v for k, v in dic.items()}\n\n# prepare table\ndf_ = pd.DataFrame(dic.items(), columns=[\"Username\", \"Betweenness Score\"])\ndf_[\"(normalised)\"] = df_[\"Betweenness Score\"] / df_[\"Betweenness Score\"].max()\ndf_ = df_.round(5)\ndf_.index = np.arange(1, len(df_) + 1)\n\nprint(df_)\n\n\n           Username  Betweenness Score  (normalised)\n1     leseerlaubnis            0.00192       1.00000\n2      BentsCristin            0.00184       0.96216\n3          SicherEU            0.00180       0.94201\n4    EberhardSchlie            0.00165       0.86060\n5     Haselmaus2010            0.00152       0.79417\n6         Diggi_tal            0.00152       0.79140\n7      EINSBERGBLOG            0.00151       0.78866\n8       hav_hendrik            0.00126       0.65996\n9   JSchmitzLeipzig            0.00122       0.63908\n10      Herr_Nuxoll            0.00119       0.62275\n11            ciffi            0.00115       0.60136\n12    risikogruppen            0.00085       0.44283\n13     sdw_Stiftung            0.00081       0.42026\n14   HSE_Heidelberg            0.00080       0.41651\n15  zumWeitertragen            0.00072       0.37458\n16           SenBJF            0.00068       0.35302\n17    PartnerSchule            0.00059       0.30768\n18      DianaKnodel            0.00053       0.27429\n19         Woe_Real            0.00050       0.26349\n20       BerlinSoul            0.00046       0.23835\n\n\nCalculate top edges by betweenness centrality\n\n\nCode\ndic = {i: ep.a[i] for i in range(g.num_edges())}\ndic = {key: val for key, val in (sorted(dic.items(), key=lambda x: x[1], reverse=True)[:20])}\n\n# get names of tweeters and retweeters\nlis = [(list(g.ep.tweeter)[k], list(g.ep.retweeter)[k], v) for k, v in dic.items()]\n\n# prepare table\ndf_ = pd.DataFrame(lis, columns=[\"Tweeter\", \"Retweeter\", \"Betweenness Score\"])\ndf_[\"(normalised)\"] = df_[\"Betweenness Score\"] / df_[\"Betweenness Score\"].max()\ndf_ = df_.round(5)\ndf_.index = np.arange(1, len(df_) + 1)\n\nprint(df_)\n\n\n            Tweeter        Retweeter  Betweenness Score  (normalised)\n1          SicherEU     BentsCristin            0.00172       1.00000\n2      BentsCristin   EberhardSchlie            0.00165       0.96337\n3    EberhardSchlie    leseerlaubnis            0.00157       0.91803\n4     Haselmaus2010        Diggi_tal            0.00152       0.88464\n5      EINSBERGBLOG    Haselmaus2010            0.00149       0.86772\n6       Herr_Nuxoll      hav_hendrik            0.00121       0.70300\n7       hav_hendrik  JSchmitzLeipzig            0.00118       0.68927\n8             ciffi      Herr_Nuxoll            0.00115       0.67047\n9   JSchmitzLeipzig     EINSBERGBLOG            0.00097       0.56556\n10        Diggi_tal    risikogruppen            0.00081       0.47287\n11   HSE_Heidelberg     sdw_Stiftung            0.00081       0.46975\n12    risikogruppen         SicherEU            0.00075       0.43932\n13  zumWeitertragen         SicherEU            0.00072       0.42075\n14        Diggi_tal  zumWeitertragen            0.00072       0.42007\n15    PartnerSchule           SenBJF            0.00061       0.35491\n16     sdw_Stiftung    PartnerSchule            0.00059       0.34380\n17         Woe_Real            ciffi            0.00052       0.30579\n18           SenBJF     EINSBERGBLOG            0.00052       0.30050\n19  StaackSebastian  dreadlock_dread            0.00039       0.22800\n20       BerlinSoul   regina_kittler            0.00038       0.22337"
  }
]