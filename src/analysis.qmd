# Twitter Network Analysis

## Data preparation

First, let's define some helper functions to be used later.

```{python}

import json
import re

import numpy as np
import pandas as pd

def extract_missing_usernames(df, username_column):
    pattern = r"RT @([A-Za-z0-9_]+):"
    usernames = []
    for index, row in df.iterrows():
        match = re.search(pattern, row["retweet_text"])
        if match:
            usernames.append(match.group(1))
        else:
            usernames.append(row[username_column])
    df[username_column] = usernames

    return df


def get_time_range(df):
    """Get the time range of the DataFrame

    Parameters
    ----------
    df : pandas.DataFrame
        DataFrame to be queried
    date_lang : str, optional
        Language of the date, by default "de_DE"

    Returns
    -------
    str
        Start date of the DataFrame
    str
        End date of the DataFrame
    """
    df.retweet_created_at = df.retweet_created_at.astype("datetime64[ns, UTC]")
    start_date = df.retweet_created_at.min().strftime("%B %e, %Y")
    end_date = df.retweet_created_at.max().strftime("%B %e, %Y")

    return start_date, end_date


def get_largest_values(df, col_name, n):
    """Get the n largest values of a column in a DataFrame

    Parameters
    ----------
    df : pandas.DataFrame
        DataFrame to be queried
    col_name : str
        Name of the column to be queried
    n : int
        Number of largest values to be returned (i.e. number of rows)

    Returns
    -------
    pandas.DataFrame
        DataFrame with the n largest values of the column
    """
    top = df.sort_values(col_name, ascending=False).head(n)

    # put col_name as first column
    cols = top.columns.tolist()
    cols = cols[-1:] + cols[:-1]
    top = top[cols]

    return top


def get_top_users(df, df_authors, column_name, n):
    """Get the top n users with their profiles based on a column in a DataFrame
    Values of the column are standardized so that the largest value is 1.0

    Parameters
    ----------
    df : pandas.DataFrame
        DataFrame to be queried
    df_authors : pandas.DataFrame
        DataFrame with the usernames and names of authors
    column_name : str
        Name of the column to be queried
    n : int
        Number of largest values to be returned (i.e. number of rows)

    Returns
    -------
    pandas.DataFrame
        DataFrame with the n largest values of the column
    """
    column_name_std = column_name + " (normalised)"
    df[column_name_std] = df[column_name] / max(df[column_name])
    df = get_largest_values(df, column_name, n)
    df = add_profile_url(df, "username")
    df = pd.merge(df, df_authors, on="username", how="left")
    df = df.round(5)
    df.index = np.arange(1, len(df) + 1)
    df = df[[column_name, column_name_std, "username", "name", "profile_url"]]
    return df


def get_authors_name(df):
    """Get the usernames and names of retweet authors and tweet authors

    Parameters
    ----------
    df : pandas.DataFrame
        DataFrame to be queried

    Returns
    -------
    pandas.DataFrame
        DataFrame with the usernames and names of retweet authors and tweet authors
    """
    retweet_authors = df[["retweet_author_username", "retweet_author_name"]].copy()
    retweet_authors.rename(
        columns={"retweet_author_username": "username", "retweet_author_name": "name"},
        inplace=True,
    )

    tweet_authors = df[["tweet_author_username", "tweet_author_name"]].copy()
    tweet_authors.rename(
        columns={"tweet_author_username": "username", "tweet_author_name": "name"},
        inplace=True,
    )

    authors = pd.concat([retweet_authors, tweet_authors])
    authors = authors.drop_duplicates(subset=["username"], keep="last").reset_index(
        drop=True
    )

    return authors


def add_profile_url(df, username_col):
    df["profile_url"] = "https://twitter.com/" + df[username_col]

    return df
```


Now let's prepare the dataset and print some relevant information about it.

```{python}

# load and clean dataset
df = pd.read_parquet("data/raw/all_tweets_lehrkraeftebildung.parquet")
df.replace(["NaN", "nan", "None", ""], np.NaN, inplace=True)
df = extract_missing_usernames(df, "tweet_author_username")

# get information about the retweets
start_date, end_date = get_time_range(df)
search_words = "(Lehrkräftebildung OR Lehrerbildung OR Lehrkräfte OR Lehrkräftefortbildung OR Seiteneinstieg OR Quereinstieg OR Lehramt)"
query_conds = "(is:retweet OR is:quote) lang:de"

# drop retweets with missing usernames
old_df_len = df.shape[0]
try:
    missing_usernames = df.tweet_author_username.isnull().value_counts()[True]
except KeyError:
    missing_usernames = 0
df = df.dropna(subset=["tweet_author_username"])


# Print info about dataset
print(f"Number of total retweets in this dataset: \n{old_df_len}")
print(f"\nTime range of the retweets:\n{start_date} - {end_date}")
print(f"\nKeywords* used to collect the retweets:\n{search_words}")
print(f"\nQuery conditions used to collect the retweets:\n{query_conds}")
print(f"\nNumber of retweets with missing usernames for the original tweeter: {missing_usernames}\nThese are being dropped from the analysis. New total of retweets: {len(df)}")

```

Now let's prepare the graph.

```{python}
import graph_tool.all as gt

# prepare dataset for graph
df_ = df.copy()
df_["weight"] = df_.groupby(['retweet_author_username', 'tweet_author_username']).transform('size')

# choose columns to keep
columns = ['retweet_author_username',
    'tweet_author_username',
    "weight",
    ]

df_ = df_[columns].drop_duplicates(subset=['retweet_author_username', 'tweet_author_username'])

# create list of edges
g_list = [(r, t, w, r, t) for r, t, w in zip(df_.retweet_author_username, df_.tweet_author_username, df_.weight)]

# create graph
g = gt.Graph(
    g_list,
    hashed=True,
    eprops=[('weight', 'int'), ('retweeter', 'string'), ('tweeter', 'string')]
)

```


## Visualising the network

### The network as a whole

```{python}
gt.graph_draw(g)
```

### The largest component of the network

If we look at the network as a whole, we can see that there are many isolated nodes on its periphery. If we remove those, this is what we get.

```{python}
# subgraph of largest component
g = gt.extract_largest_component(g, directed=False)
g = gt.Graph(g, prune=True)
pos = gt.sfdp_layout(g, eweight=g.edge_properties["weight"])

gt.graph_draw(g, pos)

# create mapping from usernames to vertex ids
id_usernames = {v: g.vp.ids[v] for v in range(len(list(g.vp.ids)))}
```

## Statistics of the (sub)network

### Degree distribution

```{python}
import matplotlib.pyplot as plt

# in & out degree distribution

in_hist = gt.vertex_hist(g, "in")
out_hist = gt.vertex_hist(g, "out")

y = in_hist[0]
err = np.sqrt(in_hist[0])
plt.errorbar(in_hist[1][:-1], in_hist[0], fmt="o", yerr=err,
        label="in")

y = out_hist[0]
err = np.sqrt(out_hist[0])
plt.errorbar(out_hist[1][:-1], out_hist[0], fmt="o", yerr=err,
        label="out")

plt.yscale("log")
plt.xscale("log")

plt.xlabel("$k$")
plt.ylabel("$NP(k_{in})$")

plt.tight_layout()
plt.legend()
```


## Measures of centrality

### Betweenness centrality

Betweeness centrality represents the number of all ‘shortest paths’ between nodes that pass through a specific node. In other words, it counts how often that node is part of a short connection.
In the case of retweets, it measures the **extent to which a user connects different communities of users**.

- Useful when there is flux / information flow within network
- Helps predict and locate vulnerability of a network (i.e. if you remove a node with high betweeness)
- Problem: computationally expensive (as you need to calculate all shortest paths)

```{python}

vp, ep = gt.betweenness(g)

gt.graph_draw(g, pos=pos, vertex_fill_color=vp,
              vertex_size=gt.prop_to_size(vp, mi=1, ma=10),
              edge_pen_width=gt.prop_to_size(ep, mi=0.5, ma=5),
              vcmap=plt.cm.autumn,
              vorder=vp)

```

Calculate top users betweenness centrality

```{python}

dic = {i: vp.a[i] for i in range(g.num_vertices())}
dic = {key: val for key, val in (sorted(dic.items(), key=lambda x: x[1], reverse=True)[:20])}

# map names to ids
dic = {id_usernames[k]: v for k, v in dic.items()}

# prepare table
df_ = pd.DataFrame(dic.items(), columns=["Username", "Betweenness Score"])
df_["(normalised)"] = df_["Betweenness Score"] / df_["Betweenness Score"].max()
df_ = df_.round(5)
df_.index = np.arange(1, len(df_) + 1)

print(df_)

```

Calculate top edges by betweenness centrality

```{python}

dic = {i: ep.a[i] for i in range(g.num_edges())}
dic = {key: val for key, val in (sorted(dic.items(), key=lambda x: x[1], reverse=True)[:20])}

# get names of tweeters and retweeters
lis = [(list(g.ep.tweeter)[k], list(g.ep.retweeter)[k], v) for k, v in dic.items()]

# prepare table
df_ = pd.DataFrame(lis, columns=["Tweeter", "Retweeter", "Betweenness Score"])
df_["(normalised)"] = df_["Betweenness Score"] / df_["Betweenness Score"].max()
df_ = df_.round(5)
df_.index = np.arange(1, len(df_) + 1)

print(df_)

```
